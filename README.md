In this exercise, we have completed a series of tasks related to word embeddings and traditional text classification using the pre-trained word embeddings, word2vec (word2vec-google-news-300) and GloVe (glove-wiki-gigaword-300) included in gensim, and the AG News Topic Classification dataset with the scikit-learn library.

# Question 1 (Code File: Question1):
We analyzed word2vec and GloVe embeddings to find the closest words to given words, such as 'car', 'jaguar', 'Jaguar', and 'facebook', and compared the results to identify common words in the results from both embeddings. We also repeated this task with four words of our choice. Next, we found the 10 closest words to the word 'student' and provided lists of words that are not related to university students or primary, middle, or high school students. We then worked with analogies to find the closest words using both word2vec and GloVe embeddings and repeated the process with three analogies of our choice, commenting on the results.

# Question 2 (Code File: Question2):
We implemented various text classification approaches using the scikit-learn library, including Multinomial Na√Øve Bayes with tf-idf word uni-grams and character tri-grams representations, and Support Vector Machines (linear kernel, C=1) with tf-idf word uni-grams and character tri-grams representations. We reported the performance (accuracy on test set), dimensionality (number of different n-grams used), and time cost (in seconds) of these models in a table. Finally, we identified specific texts in the test set that all models misclassify, showed the contents of one such text, reported the number of such texts per category (World, Sports, Business, Sci/Tech), and identified the most common pair of correct category and wrong prediction for these texts.
