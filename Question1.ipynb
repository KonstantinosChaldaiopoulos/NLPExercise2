{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "id": "Lqasce8tLHgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLIHtZ41uQcs",
        "outputId": "f3afe4ba-c02b-4d79-9a99-53644b83673e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "[=================================================-] 99.5% 374.2/376.1MB downloaded\n",
            "Closest words to 'car' using word2vec:\n",
            "vehicle: 0.7821\n",
            "cars: 0.7424\n",
            "SUV: 0.7161\n",
            "minivan: 0.6907\n",
            "truck: 0.6736\n",
            "Car: 0.6678\n",
            "Ford_Focus: 0.6673\n",
            "Honda_Civic: 0.6627\n",
            "Jeep: 0.6511\n",
            "pickup_truck: 0.6441\n",
            "\n",
            "Closest words to 'car' using GloVe:\n",
            "cars: 0.7827\n",
            "vehicle: 0.7655\n",
            "truck: 0.7351\n",
            "driver: 0.7115\n",
            "driving: 0.6442\n",
            "vehicles: 0.6328\n",
            "motorcycle: 0.6023\n",
            "automobile: 0.5956\n",
            "parked: 0.5910\n",
            "drivers: 0.5778\n",
            "\n",
            "Common words for 'car':\n",
            "{'truck', 'cars', 'vehicle'}\n",
            "\n",
            "Closest words to 'jaguar' using word2vec:\n",
            "jaguars: 0.6738\n",
            "Macho_B: 0.6313\n",
            "panther: 0.6086\n",
            "lynx: 0.5815\n",
            "rhino: 0.5754\n",
            "lizard: 0.5607\n",
            "tapir: 0.5563\n",
            "tiger: 0.5529\n",
            "leopard: 0.5473\n",
            "Florida_panther: 0.5464\n",
            "\n",
            "Closest words to 'jaguar' using GloVe:\n",
            "rover: 0.5931\n",
            "bmw: 0.5415\n",
            "mercedes: 0.5256\n",
            "sepecat: 0.5030\n",
            "mustang: 0.4987\n",
            "lexus: 0.4845\n",
            "volvo: 0.4829\n",
            "cosworth: 0.4809\n",
            "xk: 0.4764\n",
            "maserati: 0.4757\n",
            "\n",
            "Common words for 'jaguar':\n",
            "set()\n",
            "\n",
            "Closest words to 'Jaguar' using word2vec:\n",
            "Land_Rover: 0.6484\n",
            "Aston_Martin: 0.6437\n",
            "Mercedes: 0.6420\n",
            "Porsche: 0.6233\n",
            "BMW: 0.6055\n",
            "Bentley_Arnage: 0.6040\n",
            "XF_sedan: 0.5996\n",
            "Audi: 0.5976\n",
            "Jaguar_XF: 0.5951\n",
            "XJ_saloon: 0.5942\n",
            "\n",
            "Closest words to 'Jaguar' using GloVe:\n",
            "Word 'Jaguar' not found in GloVe model.\n",
            "Unable to find common words for 'Jaguar' as it was not found in one or both models.\n",
            "\n",
            "Closest words to 'facebook' using word2vec:\n",
            "Facebook: 0.7564\n",
            "FaceBook: 0.7077\n",
            "twitter: 0.6989\n",
            "myspace: 0.6942\n",
            "Twitter: 0.6642\n",
            "twitter_facebook: 0.6572\n",
            "Facebook.com: 0.6530\n",
            "myspace_facebook: 0.6371\n",
            "facebook_twitter: 0.6368\n",
            "linkedin: 0.6357\n",
            "\n",
            "Closest words to 'facebook' using GloVe:\n",
            "twitter: 0.8350\n",
            "myspace: 0.8056\n",
            "youtube: 0.7292\n",
            "blog: 0.6404\n",
            "linkedin: 0.6333\n",
            "google: 0.6268\n",
            "website: 0.6157\n",
            "web: 0.6143\n",
            "blogs: 0.6064\n",
            "networking: 0.6047\n",
            "\n",
            "Common words for 'facebook':\n",
            "{'linkedin', 'twitter', 'myspace'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "words = ['car', 'jaguar', 'Jaguar', 'facebook']\n",
        "\n",
        "for word in words:\n",
        "    try:\n",
        "        print(f\"Closest words to '{word}' using word2vec:\")\n",
        "        w2v_closest = word2vec_model.most_similar(positive=[word], topn=10)\n",
        "        for w, similarity in w2v_closest:\n",
        "            print(f\"{w}: {similarity:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"Word '{word}' not found in word2vec model.\")\n",
        "        w2v_closest = []\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nClosest words to '{word}' using GloVe:\")\n",
        "        glove_closest = glove_model.most_similar(positive=[word], topn=10)\n",
        "        for w, similarity in glove_closest:\n",
        "            print(f\"{w}: {similarity:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"Word '{word}' not found in GloVe model.\")\n",
        "        glove_closest = []\n",
        "\n",
        "    # Find common words\n",
        "    if w2v_closest and glove_closest:\n",
        "        w2v_words = set([w for w, _ in w2v_closest])\n",
        "        glove_words = set([w for w, _ in glove_closest])\n",
        "        common_words = w2v_words.intersection(glove_words)\n",
        "\n",
        "        print(f\"\\nCommon words for '{word}':\")\n",
        "        print(common_words, end=\"\\n\\n\")\n",
        "    else:\n",
        "        print(f\"Unable to find common words for '{word}' as it was not found in one or both models.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = ['fugue', 'scale', 'harmony', 'consciousness']\n",
        "\n",
        "for word in words:\n",
        "    try:\n",
        "        print(f\"Closest words to '{word}' using word2vec:\")\n",
        "        w2v_closest = word2vec_model.most_similar(positive=[word], topn=10)\n",
        "        for w, similarity in w2v_closest:\n",
        "            print(f\"{w}: {similarity:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"Word '{word}' not found in word2vec model.\")\n",
        "        w2v_closest = []\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nClosest words to '{word}' using GloVe:\")\n",
        "        glove_closest = glove_model.most_similar(positive=[word], topn=10)\n",
        "        for w, similarity in glove_closest:\n",
        "            print(f\"{w}: {similarity:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"Word '{word}' not found in GloVe model.\")\n",
        "        glove_closest = []\n",
        "\n",
        "    \n",
        "    if w2v_closest and glove_closest:\n",
        "        w2v_words = set([w for w, _ in w2v_closest])\n",
        "        glove_words = set([w for w, _ in glove_closest])\n",
        "        common_words = w2v_words.intersection(glove_words)\n",
        "\n",
        "        print(f\"\\nCommon words for '{word}':\")\n",
        "        print(common_words, end=\"\\n\\n\")\n",
        "    else:\n",
        "        print(f\"Unable to find common words for '{word}' as it was not found in one or both models.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98zn9WMsyzh6",
        "outputId": "bd545f3c-2954-4754-ce1b-7ff5550ce178"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest words to 'fugue' using word2vec:\n",
            "sonata: 0.6648\n",
            "cadenza: 0.6632\n",
            "scherzo: 0.6601\n",
            "fugal: 0.6513\n",
            "toccata: 0.6430\n",
            "Liebestod: 0.6306\n",
            "Scherzo: 0.6302\n",
            "fugues: 0.6274\n",
            "adagio: 0.6274\n",
            "allegretto: 0.6220\n",
            "\n",
            "Closest words to 'fugue' using GloVe:\n",
            "toccata: 0.5986\n",
            "passacaglia: 0.5491\n",
            "fugues: 0.5455\n",
            "counterpoint: 0.5092\n",
            "scherzo: 0.4866\n",
            "concerto: 0.4848\n",
            "op.: 0.4754\n",
            "prelude: 0.4737\n",
            "sonata: 0.4704\n",
            "harpsichord: 0.4608\n",
            "\n",
            "Common words for 'fugue':\n",
            "{'toccata', 'fugues', 'sonata', 'scherzo'}\n",
            "\n",
            "Closest words to 'scale' using word2vec:\n",
            "large_scale: 0.6020\n",
            "ruAA_Russia: 0.4902\n",
            "Likert_type: 0.4872\n",
            "Scale: 0.4772\n",
            "integrate_CyVera_technology: 0.4721\n",
            "scales: 0.4575\n",
            "scope: 0.4562\n",
            "Happiness_quotient: 0.4512\n",
            "Likert_pain: 0.4499\n",
            "elaborately_staged_displays: 0.4488\n",
            "\n",
            "Closest words to 'scale' using GloVe:\n",
            "magnitude: 0.5676\n",
            "massive: 0.5452\n",
            "measuring: 0.5242\n",
            "large-scale: 0.4953\n",
            "large: 0.4921\n",
            "measured: 0.4806\n",
            "earthquake: 0.4682\n",
            "larger: 0.4653\n",
            "extent: 0.4602\n",
            "scales: 0.4583\n",
            "\n",
            "Common words for 'scale':\n",
            "{'scales'}\n",
            "\n",
            "Closest words to 'harmony' using word2vec:\n",
            "harmonious: 0.7439\n",
            "harmoniously: 0.6587\n",
            "amity: 0.6346\n",
            "peaceful_coexistence: 0.5967\n",
            "unity: 0.5903\n",
            "tranquility: 0.5895\n",
            "oneness: 0.5721\n",
            "harmonious_coexistence: 0.5689\n",
            "communal_harmony: 0.5564\n",
            "togetherness: 0.5499\n",
            "\n",
            "Closest words to 'harmony' using GloVe:\n",
            "harmonious: 0.5663\n",
            "unity: 0.5212\n",
            "prosperity: 0.4855\n",
            "counterpoint: 0.4738\n",
            "stability: 0.4683\n",
            "tranquility: 0.4545\n",
            "harmonies: 0.4521\n",
            "equality: 0.4427\n",
            "coexistence: 0.4427\n",
            "melody: 0.4125\n",
            "\n",
            "Common words for 'harmony':\n",
            "{'unity', 'harmonious', 'tranquility'}\n",
            "\n",
            "Closest words to 'consciousness' using word2vec:\n",
            "conciousness: 0.7369\n",
            "consciouness: 0.6435\n",
            "collective_consciousness: 0.5951\n",
            "consciousnesses: 0.5579\n",
            "unconsciousness: 0.5403\n",
            "consiousness: 0.5028\n",
            "discourse: 0.4998\n",
            "Consciousness: 0.4836\n",
            "psyche: 0.4829\n",
            "causes_convulsions_paralysis: 0.4733\n",
            "\n",
            "Closest words to 'consciousness' using GloVe:\n",
            "conscious: 0.5381\n",
            "unconscious: 0.5049\n",
            "mind: 0.4929\n",
            "awareness: 0.4821\n",
            "senses: 0.4814\n",
            "perception: 0.4794\n",
            "cognition: 0.4685\n",
            "regaining: 0.4612\n",
            "discourse: 0.4565\n",
            "awaken: 0.4383\n",
            "\n",
            "Common words for 'consciousness':\n",
            "{'discourse'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word = 'student'\n",
        "\n",
        "\n",
        "print(f\"Closest words to '{word}' using word2vec:\")\n",
        "w2v_closest = word2vec_model.most_similar(positive=['student'], topn=10)\n",
        "for w, similarity in w2v_closest:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nClosest words to '{word}' using GloVe:\")\n",
        "glove_closest = glove_model.most_similar(positive=[word], topn=10)\n",
        "for w, similarity in glove_closest:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nClosest words to '{word}' not related to university students using word2vec:\")\n",
        "w2v_not_university = word2vec_model.most_similar(positive=[word], negative=['university'], topn=10)\n",
        "for w, similarity in w2v_not_university:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nClosest words to '{word}' not related to university students using GloVe:\")\n",
        "glove_not_university = glove_model.most_similar(positive=[word], negative=['university'], topn=10)\n",
        "for w, similarity in glove_not_university:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nClosest words to '{word}' not related to Primary, Secondary, or High school students using word2vec:\")\n",
        "w2v_not_school = word2vec_model.most_similar(positive=[word], negative=['Primary_School','Secondary_School','High_School'], topn=10)\n",
        "\n",
        "for w, similarity in w2v_not_school:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nThe words primary_school, secondary_school, elementary_school do not exist in GloVe, we are going to try something similar.\")    \n",
        "\n",
        "print(f\"\\nClosest words to '{word}' not related to primary, secondary, or high  using GloVe:\")\n",
        "glove_not_school = glove_model.most_similar(positive=[word], negative=['primary','secondary','high'], topn=10)\n",
        "for w, similarity in glove_not_school:\n",
        "    print(f\"{w}: {similarity:.4f}\")    \n",
        "\n",
        "print(f\"\\nClosest words to '{word}' and 'school' not related to primary, secondary, or high using GloVe:\")\n",
        "glove_not_school = glove_model.most_similar(positive=[word,'school'], negative=['primary','secondary','high'], topn=10)\n",
        "for w, similarity in glove_not_school:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xz_rJiX19lv",
        "outputId": "74c0b20d-5378-4498-c2d1-a9fb20ea9c63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest words to 'student' using word2vec:\n",
            "students: 0.7295\n",
            "Student: 0.6707\n",
            "teacher: 0.6301\n",
            "stu_dent: 0.6241\n",
            "faculty: 0.6087\n",
            "school: 0.6056\n",
            "undergraduate: 0.6020\n",
            "university: 0.6005\n",
            "undergraduates: 0.5756\n",
            "semester: 0.5738\n",
            "\n",
            "Closest words to 'student' using GloVe:\n",
            "students: 0.7691\n",
            "teacher: 0.6874\n",
            "graduate: 0.6738\n",
            "school: 0.6131\n",
            "college: 0.6090\n",
            "undergraduate: 0.6044\n",
            "faculty: 0.5999\n",
            "university: 0.5971\n",
            "academic: 0.5810\n",
            "campus: 0.5768\n",
            "\n",
            "Closest words to 'student' not related to university students using word2vec:\n",
            "sixth_grader: 0.4324\n",
            "seventh_grader: 0.4178\n",
            "8th_grader: 0.4173\n",
            "eighth_grader: 0.4082\n",
            "grader: 0.3971\n",
            "kindergartner: 0.3918\n",
            "kindergartener: 0.3777\n",
            "Kindergartner: 0.3565\n",
            "teen: 0.3470\n",
            "middle_schooler: 0.3384\n",
            "\n",
            "Closest words to 'student' not related to university students using GloVe:\n",
            "15-year: 0.3830\n",
            "16-year: 0.3815\n",
            "17-year: 0.3785\n",
            "14-year: 0.3766\n",
            "13-year-old: 0.3730\n",
            "14-year-old: 0.3676\n",
            "9-year: 0.3667\n",
            "16-year-old: 0.3615\n",
            "15-year-old: 0.3510\n",
            "12-year-old: 0.3490\n",
            "\n",
            "Closest words to 'student' not related to Primary, Secondary, or High school students using word2vec:\n",
            "Tarina_Keene_executive: 0.2352\n",
            "Joseph_Belth: 0.2322\n",
            "Patricia_Metting: 0.2303\n",
            "Sarahi_Uribe: 0.2276\n",
            "Sheila_Attwood: 0.2252\n",
            "Erick_Egger: 0.2224\n",
            "James_Gerometta: 0.2172\n",
            "Nick_Anthis: 0.2153\n",
            "Jesse_Masyr: 0.2146\n",
            "Julie_Zied: 0.2119\n",
            "\n",
            "The words primary_school, secondary_school, elementary_school do not exist in GloVe, we are going to try something similar.\n",
            "\n",
            "Closest words to 'student' not related to primary, secondary, or high  using GloVe:\n",
            "farzaneh: 0.3924\n",
            "mohmmed: 0.3902\n",
            "pitambar: 0.3902\n",
            "byyny: 0.3862\n",
            "k586-1: 0.3813\n",
            "stucki: 0.3778\n",
            "gassama: 0.3770\n",
            "kudirka: 0.3760\n",
            "-3.50: 0.3742\n",
            "lanzillo: 0.3740\n",
            "\n",
            "Closest words to 'student' and 'school' not related to primary, secondary, or high using GloVe:\n",
            "22-year-old: 0.3562\n",
            "roommate: 0.3321\n",
            "21-year-old: 0.3294\n",
            "byyny: 0.3251\n",
            "23-year-old: 0.3175\n",
            "suzana: 0.3163\n",
            "befriended: 0.3141\n",
            "stucki: 0.3128\n",
            "20-year-old: 0.3105\n",
            "yaqut: 0.3095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nking - man + woman using GloVe\")\n",
        "glove_test = glove_model.most_similar(positive=['king','woman'], negative=['man'], topn=10)\n",
        "for w, similarity in glove_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nking - man + woman using word2vec\")\n",
        "word2vec_test  = word2vec_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=10)\n",
        "for w, similarity in word2vec_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWGz6KV2U45a",
        "outputId": "437a2363-1f6b-48af-a2d6-7d67dfb1b38c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "king - man + woman using GloVe\n",
            "queen: 0.6713\n",
            "princess: 0.5433\n",
            "throne: 0.5386\n",
            "monarch: 0.5348\n",
            "daughter: 0.4980\n",
            "mother: 0.4956\n",
            "elizabeth: 0.4833\n",
            "kingdom: 0.4775\n",
            "prince: 0.4668\n",
            "wife: 0.4647\n",
            "\n",
            "king - man + woman using word2vec\n",
            "queen: 0.7118\n",
            "monarch: 0.6190\n",
            "princess: 0.5902\n",
            "crown_prince: 0.5499\n",
            "prince: 0.5377\n",
            "kings: 0.5237\n",
            "Queen_Consort: 0.5236\n",
            "queens: 0.5181\n",
            "sultan: 0.5099\n",
            "monarchy: 0.5087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nfrance - paris + tokyo using GloVe\")\n",
        "glove_test = glove_model.most_similar(positive=['france','tokyo'], negative=['paris'], topn=10)\n",
        "for w, similarity in glove_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nFrance - Paris + Tokyo using word2vec\")\n",
        "word2vec_test  = word2vec_model.most_similar(positive=['France', 'Tokyo'], negative=['Paris'], topn=10)\n",
        "for w, similarity in word2vec_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McBKoayrXu16",
        "outputId": "13cd14a4-ea2c-4251-fc78-a996a6b9da66"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "france - paris + tokyo using GloVe\n",
            "japan: 0.8017\n",
            "japanese: 0.6111\n",
            "korea: 0.5508\n",
            "yen: 0.4853\n",
            "taiwan: 0.4487\n",
            "australia: 0.4362\n",
            "china: 0.4314\n",
            "germany: 0.4263\n",
            "koizumi: 0.4212\n",
            "zealand: 0.4178\n",
            "\n",
            "France - Paris + Tokyo using word2vec\n",
            "Japan: 0.8168\n",
            "Japanese: 0.6481\n",
            "South_Korea: 0.6142\n",
            "Japans: 0.6117\n",
            "Shizuoka: 0.5742\n",
            "Aomori_Prefecture: 0.5598\n",
            "northernmost_prefecture: 0.5525\n",
            "Kyushu: 0.5514\n",
            "captain_Makoto_Hasebe: 0.5508\n",
            "Shimane: 0.5497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\ntrees - apples + grapes using GloVe\")\n",
        "glove_test = glove_model.most_similar(positive=['trees','grapes'], negative=['apples'], topn=10)\n",
        "for w, similarity in glove_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\ntrees - apples + grapes using word2vec\")\n",
        "word2vec_test  = word2vec_model.most_similar(positive=['trees', 'grapes'], negative=['apples'], topn=10)\n",
        "for w, similarity in word2vec_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur6rFdqEYKDF",
        "outputId": "fd7aa645-f033-4939-965c-86428134329b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "trees - apples + grapes using GloVe\n",
            "vines: 0.5909\n",
            "tree: 0.5843\n",
            "planted: 0.5468\n",
            "forests: 0.5134\n",
            "grape: 0.4985\n",
            "shrubs: 0.4978\n",
            "vineyards: 0.4902\n",
            "vineyard: 0.4754\n",
            "vegetation: 0.4743\n",
            "oak: 0.4643\n",
            "\n",
            "trees - apples + grapes using word2vec\n",
            "oak_trees: 0.6750\n",
            "vines: 0.6702\n",
            "pine_trees: 0.6573\n",
            "oaks: 0.6505\n",
            "tree: 0.6358\n",
            "hardwood_trees: 0.5963\n",
            "sycamore_trees: 0.5949\n",
            "tress: 0.5849\n",
            "beech_trees: 0.5817\n",
            "locust_trees: 0.5807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nswimming - walking + walked using GloVe\")\n",
        "glove_test = glove_model.most_similar(positive=['swimming','walking'], negative=['walked'], topn=10)\n",
        "for w, similarity in glove_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nswimming - walking + walked using word2vec\")\n",
        "word2vec_test  = word2vec_model.most_similar(positive=['swimming', 'walking'], negative=['walked'], topn=10)\n",
        "for w, similarity in word2vec_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sASurNEhYeqa",
        "outputId": "dcf03666-ea0a-4d8c-cf5b-988c9e53364e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "swimming - walking + walked using GloVe\n",
            "biking: 0.6108\n",
            "swim: 0.5915\n",
            "canoeing: 0.5557\n",
            "skiing: 0.5424\n",
            "surfing: 0.5401\n",
            "kayaking: 0.5360\n",
            "rowing: 0.5287\n",
            "bicycling: 0.5280\n",
            "hiking: 0.5085\n",
            "boating: 0.5049\n",
            "\n",
            "swimming - walking + walked using word2vec\n",
            "swim: 0.6876\n",
            "Swimming: 0.6064\n",
            "paddling: 0.6053\n",
            "kayaking: 0.5901\n",
            "swimmers: 0.5894\n",
            "swims: 0.5741\n",
            "jogging_bicycling: 0.5594\n",
            "rowing: 0.5586\n",
            "swimmer: 0.5516\n",
            "biking: 0.5463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\ndoctor - father + mother using GloVe\")\n",
        "glove_test = glove_model.most_similar(positive=['doctor','father'], negative=['mother'], topn=10)\n",
        "for w, similarity in glove_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\ndoctor - father + mother using word2vec\")\n",
        "word2vec_test  = word2vec_model.most_similar(positive=['doctor', 'father'], negative=['mother'], topn=10)\n",
        "for w, similarity in word2vec_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKTKxYPLZ8N2",
        "outputId": "1644a549-5cd5-4710-aa8a-179940df0893"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "doctor - father + mother using GloVe\n",
            "physician: 0.6203\n",
            "surgeon: 0.5517\n",
            "dr.: 0.5368\n",
            "brother: 0.5206\n",
            "son: 0.4784\n",
            "pharmacist: 0.4692\n",
            "uncle: 0.4688\n",
            "he: 0.4639\n",
            "medical: 0.4595\n",
            "dentist: 0.4573\n",
            "\n",
            "doctor - father + mother using word2vec\n",
            "physician: 0.6708\n",
            "surgeon: 0.6097\n",
            "doctors: 0.5943\n",
            "orthopedic_surgeon: 0.5938\n",
            "dentist: 0.5840\n",
            "urologist: 0.5735\n",
            "cardiologist: 0.5722\n",
            "orthopedist: 0.5655\n",
            "ophthalmologist: 0.5647\n",
            "neurologist: 0.5514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nWords of my choice:\")\n",
        "print(f\"\\nawareness - consciousness + creativity using GloVe\")\n",
        "glove_test = glove_model.most_similar(positive=['doctor','father'], negative=['mother'], topn=10)\n",
        "for w, similarity in glove_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")\n",
        "\n",
        "print(f\"\\nawareness - consciousness + creativity using word2vec\")\n",
        "word2vec_test  = word2vec_model.most_similar(positive=['doctor', 'father'], negative=['mother'], topn=10)\n",
        "for w, similarity in word2vec_test:\n",
        "    print(f\"{w}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT_n_7hYaVOa",
        "outputId": "70b1b01b-8e66-460f-e414-82d0b9eb50b4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words of my choice:\n",
            "\n",
            "awareness - consciousness + creativity using GloVe\n",
            "physician: 0.6203\n",
            "surgeon: 0.5517\n",
            "dr.: 0.5368\n",
            "brother: 0.5206\n",
            "son: 0.4784\n",
            "pharmacist: 0.4692\n",
            "uncle: 0.4688\n",
            "he: 0.4639\n",
            "medical: 0.4595\n",
            "dentist: 0.4573\n",
            "\n",
            "awareness - consciousness + creativity using word2vec\n",
            "physician: 0.6708\n",
            "surgeon: 0.6097\n",
            "doctors: 0.5943\n",
            "orthopedic_surgeon: 0.5938\n",
            "dentist: 0.5840\n",
            "urologist: 0.5735\n",
            "cardiologist: 0.5722\n",
            "orthopedist: 0.5655\n",
            "ophthalmologist: 0.5647\n",
            "neurologist: 0.5514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comments on the above exercise. \n",
        "\n",
        "As we can see in the above results, both word2vec and GloVe perform well in identifying words semantically related to 'student'. In the negative words, a strange phenomenon occurs where the GloVe model manages to capture the age of the students that are not related with the negative aspect that we have chosen. So, for example, the students that are not in primary,secondary or highschool are over 18 years old as the output suggests while the opposite is true for students that are not related to the university. Under 18 years old. The outputs are quite similar for word2vec and GloVe in the other examples, but word2vec seems to capture more abstract yet not so obvious relationships when compared to Glove. In the France - Paris + Tokyo using word2vec, there is an output such as Aomori_Prerfecture with underscore in contrast to GloVe where the output is really logical and usually one word. Also, the GloVe model uses only small letters in contast to word2vec that uses both small and capital letters. Finally, on very abstract ideas like the one we used in the words of our choice, a strange phenomenon occurs where bots models produce extremely close outputs. Yet the connection of the outputs with the input is not so obvious to humans."
      ],
      "metadata": {
        "id": "7LzEb4i_AV-A"
      }
    }
  ]
}