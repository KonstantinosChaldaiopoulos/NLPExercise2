{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuPII49Y5pCM",
        "outputId": "c8ab661c-6bfa-4065-bbde-66fd1d9ae417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "zxdLGlySOxZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main block of an 1-direction RNN classifier.This code was provided for all the other blocks to be based on:\n"
      ],
      "metadata": {
        "id": "p99CuPliHdDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1qRxgRtEZ9B",
        "outputId": "2a6a1c8d-f151-4303-9785-b27a24e61e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.300\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.064\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.970\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.930\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 29.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.908\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 29.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.892\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.879\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.870\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.862\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.857\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.851\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.847\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.840\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 29.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.836\n",
            "\n",
            "Test Accuracy : 0.868\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.87      0.88      1900\n",
            "      Sports       0.93      0.94      0.93      1900\n",
            "    Business       0.85      0.81      0.83      1900\n",
            "    Sci/Tech       0.81      0.85      0.83      1900\n",
            "\n",
            "    accuracy                           0.87      7600\n",
            "   macro avg       0.87      0.87      0.87      7600\n",
            "weighted avg       0.87      0.87      0.87      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1649   69  106   76]\n",
            " [  47 1780   19   54]\n",
            " [  82   31 1546  241]\n",
            " [  84   40  154 1622]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds1, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds1.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds1 = torch.cat(Y_preds1)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds1.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds1 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds1, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds1 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds1)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds1, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds1))\n",
        "\n",
        "parameters1 = count_parameters(classifier)\n",
        "average_time_per_epoch1 = (end_time - start_time)/EPOCHS\n",
        "accuracy1 = accuracy_score(Y_actual, Y_preds1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction RNN classifier. The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Dhjyt228IY6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e96f776-8a02-4543-dc58-e5b93779210b",
        "id": "eXjKHZ-4OyDQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2147164\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.231\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.998\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.928\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.895\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.875\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.861\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.851\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.825\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.821\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.818\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.815\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.812\n",
            "\n",
            "Test Accuracy : 0.877\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.86      0.89      1900\n",
            "      Sports       0.93      0.95      0.94      1900\n",
            "    Business       0.83      0.85      0.84      1900\n",
            "    Sci/Tech       0.83      0.86      0.84      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1633   70  103   94]\n",
            " [  23 1799   30   48]\n",
            " [  70   24 1610  196]\n",
            " [  55   33  186 1626]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds2, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds2.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds2 = torch.cat(Y_preds2)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds2.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds2 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds2, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds2 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds2)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds2, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds2))\n",
        "\n",
        "parameters2 = count_parameters(classifier)\n",
        "average_time_per_epoch2 = (end_time - start_time)/EPOCHS\n",
        "accuracy2 = accuracy_score(Y_actual, Y_preds2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction RNN classifier with 2 layers.The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zSfxdHl6JAV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c19fc0c-9493-4bfc-8f6c-92928d45d4de",
        "id": "VnjXOao7TDPQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2171996\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.209\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.000\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.932\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.900\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.880\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.866\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.856\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.848\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.842\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.836\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.828\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.824\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.821\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.819\n",
            "\n",
            "Test Accuracy : 0.877\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.90      0.86      0.88      1900\n",
            "      Sports       0.92      0.96      0.94      1900\n",
            "    Business       0.86      0.81      0.84      1900\n",
            "    Sci/Tech       0.82      0.88      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1639   65   97   99]\n",
            " [  36 1817   14   33]\n",
            " [  80   49 1544  227]\n",
            " [  59   38  135 1668]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds3, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds3.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds3 = torch.cat(Y_preds3)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds3.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds3 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds3, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds3 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds3)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds3, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds3))\n",
        "\n",
        "parameters3 = count_parameters(classifier)\n",
        "average_time_per_epoch3 = (end_time - start_time)/EPOCHS\n",
        "accuracy3 = accuracy_score(Y_actual, Y_preds3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a one-direction LSTM classifier.The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1TVZSIwuJnw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b314918e-b9a3-4c6e-b176-d02491c328cd",
        "id": "KFMUmOY6WjcP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2168156\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.251\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.975\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.911\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.883\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.864\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.852\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.826\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.822\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.819\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.816\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.814\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.811\n",
            "\n",
            "Test Accuracy : 0.880\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.91      0.88      0.89      1900\n",
            "      Sports       0.92      0.95      0.93      1900\n",
            "    Business       0.86      0.83      0.84      1900\n",
            "    Sci/Tech       0.84      0.86      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1666   74   91   69]\n",
            " [  26 1807   32   35]\n",
            " [  75   39 1572  214]\n",
            " [  72   46  142 1640]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1 direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import time \n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds4, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds4.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds4 = torch.cat(Y_preds4)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds4.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds4 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds4, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds4 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds4)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds4, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds4))\n",
        "\n",
        "parameters4 = count_parameters(classifier)\n",
        "average_time_per_epoch4 = (end_time - start_time)/EPOCHS\n",
        "accuracy4 = accuracy_score(Y_actual, Y_preds4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction LSTM classifier. The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "aJBsTT1hJ3wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds5, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds5.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds5 = torch.cat(Y_preds5)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds5.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds5 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds5, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds5 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds5)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds5, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds5))\n",
        "\n",
        "\n",
        "parameters5 = count_parameters(classifier)\n",
        "average_time_per_epoch5 = (end_time - start_time)/EPOCHS\n",
        "accuracy5 = accuracy_score(Y_actual, Y_preds5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOEk-PtaUS8",
        "outputId": "c658719a-a2aa-4b18-9c52-d41ff9833ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2210908\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.200\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.936\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.884\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.861\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.847\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.836\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.828\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.823\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.817\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.810\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.805\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.803\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.802\n",
            "\n",
            "Test Accuracy : 0.886\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.87      0.89      1900\n",
            "      Sports       0.93      0.95      0.94      1900\n",
            "    Business       0.85      0.85      0.85      1900\n",
            "    Sci/Tech       0.85      0.87      0.86      1900\n",
            "\n",
            "    accuracy                           0.89      7600\n",
            "   macro avg       0.89      0.89      0.89      7600\n",
            "weighted avg       0.89      0.89      0.89      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1649   71  108   72]\n",
            " [  25 1811   30   34]\n",
            " [  62   30 1620  188]\n",
            " [  65   25  156 1654]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction LSTM classifier with 2 layers. The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) ## Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "M1a80zEpKOSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds6, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds6.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds6 = torch.cat(Y_preds6)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds6.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds6 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds6, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds6 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds6)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds6, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds6))\n",
        "\n",
        "parameters6 = count_parameters(classifier)\n",
        "average_time_per_epoch6 = (end_time - start_time)/EPOCHS\n",
        "accuracy6 = accuracy_score(Y_actual, Y_preds6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22f2891-7d55-48b8-a9f4-4d7123b0273e",
        "id": "E3it53CzfhbY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2310236\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.148\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 16.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.932\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.888\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.867\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.853\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.844\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.838\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.826\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 16.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.822\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.817\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.814\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.812\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.810\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "\n",
            "Test Accuracy : 0.890\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.87      0.89      1900\n",
            "      Sports       0.94      0.95      0.94      1900\n",
            "    Business       0.86      0.85      0.86      1900\n",
            "    Sci/Tech       0.84      0.89      0.86      1900\n",
            "\n",
            "    accuracy                           0.89      7600\n",
            "   macro avg       0.89      0.89      0.89      7600\n",
            "weighted avg       0.89      0.89      0.89      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1652   65   93   90]\n",
            " [  27 1807   26   40]\n",
            " [  65   32 1619  184]\n",
            " [  51   27  135 1687]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the answer to the second part of question 3 which displays the results of text that were missclasified by all models and the most frequent pair of correct category and wrong prediction. The matrix of the accuracy, parameters and time cost per epoch of all the above models is below this block.\n",
        "\n"
      ],
      "metadata": {
        "id": "_VF60ceZI9eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = list(zip(Y_preds1, Y_preds2, Y_preds3, Y_preds4, Y_preds5, Y_preds6))\n",
        "\n",
        "\n",
        "misclassified_indices = []\n",
        "for i, (pred1, pred2, pred3, pred4, pred5, pred6) in enumerate(predictions):\n",
        "    if (pred1 != Y_actual[i]) and (pred2 != Y_actual[i]) and (pred3 != Y_actual[i]) and (pred4 != Y_actual[i]) and (pred5 != Y_actual[i]) and (pred6 != Y_actual[i]):\n",
        "        misclassified_indices.append(i)\n",
        "\n",
        "\n",
        "misclassified_counts = {category: 0 for category in np.unique(Y_actual)}\n",
        "\n",
        "pair_counts = {}\n",
        "\n",
        "for idx in misclassified_indices:\n",
        "    correct_category = Y_actual[idx]\n",
        "    misclassified_counts[correct_category] += 1\n",
        "\n",
        "    pair = (correct_category, Y_preds1[idx]) \n",
        "\n",
        "    if pair in pair_counts:\n",
        "        pair_counts[pair] += 1\n",
        "    else:\n",
        "        pair_counts[pair] = 1\n",
        "\n",
        "label_to_category = {\n",
        "    \"0\": \"0:World\",\n",
        "    \"1\": \"1:Sports\",\n",
        "    \"2\": \"2:Business\",\n",
        "    \"3\": \"3:Sci/Tech\",\n",
        "}\n",
        "\n",
        "misclassified_index = misclassified_indices[0]\n",
        "\n",
        "\n",
        "misclassified_text = test_dataset[misclassified_index][1]\n",
        "\n",
        "\n",
        "correct_category = Y_actual[misclassified_index]\n",
        "predicted_category = Y_preds1[misclassified_index]\n",
        "\n",
        "\n",
        "print(f\"Misclassified Text (Index: {misclassified_index}):\")\n",
        "print(misclassified_text)\n",
        "print(f\"\\nShould have been classified as: {label_to_category[str(correct_category)]}\")\n",
        "print(f\"\\nWas classified as: {label_to_category[str(predicted_category)]}\\n\")\n",
        "\n",
        "for category in sorted(misclassified_counts.keys()):\n",
        "    category_name = label_to_category[str(category)]\n",
        "    count = misclassified_counts[category]\n",
        "    print(f\"{category_name}: {count} samples\")\n",
        "\n",
        "\n",
        "most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "print(\"\\nThe most frequent pair of correct category and wrong prediction:\")\n",
        "print(f\"Correct category: {label_to_category[str(most_frequent_pair[0])]}, Wrong prediction: {label_to_category[str(most_frequent_pair[1])]}, Occurrences: {pair_counts[most_frequent_pair]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pppj9_OcvpxL",
        "outputId": "25469d52-6d6a-4426-a7f9-b3630a8638b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Text (Index: 9):\n",
            "Card fraud unit nets 36,000 cards In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\n",
            "\n",
            "Should have been classified as: 3:Sci/Tech\n",
            "\n",
            "Was classified as: 0:World\n",
            "\n",
            "0:World: 113 samples\n",
            "1:Sports: 12 samples\n",
            "2:Business: 119 samples\n",
            "3:Sci/Tech: 79 samples\n",
            "\n",
            "The most frequent pair of correct category and wrong prediction:\n",
            "Correct category: 2:Business, Wrong prediction: 3:Sci/Tech, Occurrences: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results, we can observe that the 1-layer RNN has the lowest accuracy at 0.868.  The 2-layer bidirectional LSTM having the highest accuracy at 0.890.The bidirectional models, both 1-layer and 2-layer, outperform their unidirectional counterparts, highlighting the benefits of using bidirectional architectures for capturing information from both past and future contexts. As we move from simpler architectures like RNNs to more sophisticated ones like bidirectional LSTMs, the number of parameters increases.Also, the time cost per epoch increases as we progress from simpler to more complex architectures. "
      ],
      "metadata": {
        "id": "agTy76RMLg2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_values(value):\n",
        "    if isinstance(value, float):\n",
        "        if value.is_integer():\n",
        "            return f\"{int(value):,}\"\n",
        "        else:\n",
        "            return f\"{value:.3f}\"\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"1RNN\": [accuracy1, parameters1, average_time_per_epoch1],\n",
        "    \"1-Bi-RNN\": [accuracy2, parameters2, average_time_per_epoch2],\n",
        "    \"2-Bi-RNN\": [accuracy3, parameters3, average_time_per_epoch3],\n",
        "    \"1LSTM\": [accuracy4, parameters4, average_time_per_epoch4],\n",
        "    \"1Bi-LSTM\": [accuracy5, parameters5, average_time_per_epoch5],\n",
        "    \"2Bi-LSTM\": [accuracy6, parameters6, average_time_per_epoch6],\n",
        "}\n",
        "\n",
        "index = [\"Accuracy (%)\", \"Parameters\", \"Time cost per epoch (s)\"]\n",
        "\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "df = df.applymap(format_values)\n",
        "\n",
        "df = df.style \\\n",
        "    .set_properties(**{'font-weight': 'bold', 'border': '2px solid black'}) \\\n",
        "    .set_table_styles([dict(selector='th', props=[('font-weight', 'bold'), ('border', '1px solid black')])])\n",
        "\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "_eZiZOZcLABh",
        "outputId": "4ec98232-9fd7-406a-c6f7-f0215a2cfb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f3f89a1ab80>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_7211c th {\n",
              "  font-weight: bold;\n",
              "  border: 1px solid black;\n",
              "}\n",
              "#T_7211c_row0_col0, #T_7211c_row0_col1, #T_7211c_row0_col2, #T_7211c_row0_col3, #T_7211c_row0_col4, #T_7211c_row0_col5, #T_7211c_row1_col0, #T_7211c_row1_col1, #T_7211c_row1_col2, #T_7211c_row1_col3, #T_7211c_row1_col4, #T_7211c_row1_col5, #T_7211c_row2_col0, #T_7211c_row2_col1, #T_7211c_row2_col2, #T_7211c_row2_col3, #T_7211c_row2_col4, #T_7211c_row2_col5 {\n",
              "  font-weight: bold;\n",
              "  border: 2px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_7211c\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_7211c_level0_col0\" class=\"col_heading level0 col0\" >1RNN</th>\n",
              "      <th id=\"T_7211c_level0_col1\" class=\"col_heading level0 col1\" >1-Bi-RNN</th>\n",
              "      <th id=\"T_7211c_level0_col2\" class=\"col_heading level0 col2\" >2-Bi-RNN</th>\n",
              "      <th id=\"T_7211c_level0_col3\" class=\"col_heading level0 col3\" >1LSTM</th>\n",
              "      <th id=\"T_7211c_level0_col4\" class=\"col_heading level0 col4\" >1Bi-LSTM</th>\n",
              "      <th id=\"T_7211c_level0_col5\" class=\"col_heading level0 col5\" >2Bi-LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_7211c_level0_row0\" class=\"row_heading level0 row0\" >Accuracy (%)</th>\n",
              "      <td id=\"T_7211c_row0_col0\" class=\"data row0 col0\" >0.868</td>\n",
              "      <td id=\"T_7211c_row0_col1\" class=\"data row0 col1\" >0.877</td>\n",
              "      <td id=\"T_7211c_row0_col2\" class=\"data row0 col2\" >0.877</td>\n",
              "      <td id=\"T_7211c_row0_col3\" class=\"data row0 col3\" >0.880</td>\n",
              "      <td id=\"T_7211c_row0_col4\" class=\"data row0 col4\" >0.886</td>\n",
              "      <td id=\"T_7211c_row0_col5\" class=\"data row0 col5\" >0.890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7211c_level0_row1\" class=\"row_heading level0 row1\" >Parameters</th>\n",
              "      <td id=\"T_7211c_row1_col0\" class=\"data row1 col0\" >2,136,284</td>\n",
              "      <td id=\"T_7211c_row1_col1\" class=\"data row1 col1\" >2,147,164</td>\n",
              "      <td id=\"T_7211c_row1_col2\" class=\"data row1 col2\" >2,171,996</td>\n",
              "      <td id=\"T_7211c_row1_col3\" class=\"data row1 col3\" >2,168,156</td>\n",
              "      <td id=\"T_7211c_row1_col4\" class=\"data row1 col4\" >2,210,908</td>\n",
              "      <td id=\"T_7211c_row1_col5\" class=\"data row1 col5\" >2,310,236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7211c_level0_row2\" class=\"row_heading level0 row2\" >Time cost per epoch (s)</th>\n",
              "      <td id=\"T_7211c_row2_col0\" class=\"data row2 col0\" >4.684</td>\n",
              "      <td id=\"T_7211c_row2_col1\" class=\"data row2 col1\" >4.749</td>\n",
              "      <td id=\"T_7211c_row2_col2\" class=\"data row2 col2\" >5.224</td>\n",
              "      <td id=\"T_7211c_row2_col3\" class=\"data row2 col3\" >5.107</td>\n",
              "      <td id=\"T_7211c_row2_col4\" class=\"data row2 col4\" >5.800</td>\n",
              "      <td id=\"T_7211c_row2_col5\" class=\"data row2 col5\" >6.716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}