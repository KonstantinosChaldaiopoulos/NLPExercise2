{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the file whose main focus is to use the pre-trained GloVe embeddings without freezing them."
      ],
      "metadata": {
        "id": "qXkXab-GiJ3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuPII49Y5pCM",
        "outputId": "3e4257e3-1760-4363-fb3e-8e512d1c53ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.vocab import GloVe\n"
      ],
      "metadata": {
        "id": "zxdLGlySOxZ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initialization of the embedding layer using pre-trained GloVe embeddings for the words in the vocabulary.  For words not found in the GloVe embeddings, it assigns them random embeddings within a specified range. The function also prints the number of words for which pre-trained embeddings were found out of the total words in the vocabulary. From the previous files, there is only the bellow differences. If one wants to see the differences in the construction of the different models, one should consult the previous files. "
      ],
      "metadata": {
        "id": "pKdINvDyePt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_embedding_layer(embedding_layer, glove_embeddings, vocab):\n",
        "    pretrained_weights = torch.zeros((len(vocab), EMBEDDING_DIM))\n",
        "    found_words = 0\n",
        "    for idx, word in enumerate(vocab.get_itos()):\n",
        "        if word in glove_embeddings.stoi:\n",
        "            pretrained_weights[idx] = glove_embeddings[word]\n",
        "            found_words += 1\n",
        "        else:\n",
        "            pretrained_weights[idx] = torch.FloatTensor(EMBEDDING_DIM).uniform_(-0.6, 0.6)\n",
        "\n",
        "    embedding_layer.weight.data.copy_(pretrained_weights)\n",
        "    print(f'Found {found_words} words with pre-trained embeddings out of {len(vocab)} total words.')\n"
      ],
      "metadata": {
        "id": "UZABF1Wo3wqc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In every model the following changes were made:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        ".\n",
        ".\n",
        ".\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM) # Change in code - Creates a glove object using the GloVe pre-trained word embeddings.\n",
        ".\n",
        ".\n",
        ".\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab) # Change in code - Call of the initialize_embedding_layer function withe the glove object.\n",
        ".\n",
        ".\n",
        ".\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6Mesx_t1illG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1qRxgRtEZ9B",
        "outputId": "06bc1cf2-d44a-49ae-a156-07c4f197c45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:18<00:00, 21607.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.033\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.883\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.867\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.856\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.850\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.846\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.844\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.839\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.838\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.833\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.828\n",
            "\n",
            "Test Accuracy : 0.891\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.89      0.90      1900\n",
            "      Sports       0.94      0.97      0.96      1900\n",
            "    Business       0.89      0.80      0.84      1900\n",
            "    Sci/Tech       0.82      0.91      0.86      1900\n",
            "\n",
            "    accuracy                           0.89      7600\n",
            "   macro avg       0.89      0.89      0.89      7600\n",
            "weighted avg       0.89      0.89      0.89      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1685   67   79   69]\n",
            " [  14 1849    9   28]\n",
            " [  73   33 1516  278]\n",
            " [  63   19   93 1725]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds1, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds1.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds1 = torch.cat(Y_preds1)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds1.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds1 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds1, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds1 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds1)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds1, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds1))\n",
        "\n",
        "parameters1 = count_parameters(classifier)\n",
        "average_time_per_epoch1 = (end_time - start_time)/EPOCHS\n",
        "accuracy1 = accuracy_score(Y_actual, Y_preds1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcae185-b6c5-4a7a-b7a1-1e18d432a4ee",
        "id": "eXjKHZ-4OyDQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2147164\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.990\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.858\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.834\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.827\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.821\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.817\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.812\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.806\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.804\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.803\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.801\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.800\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.798\n",
            "\n",
            "Test Accuracy : 0.904\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.91      0.90      1900\n",
            "      Sports       0.95      0.97      0.96      1900\n",
            "    Business       0.88      0.86      0.87      1900\n",
            "    Sci/Tech       0.89      0.88      0.88      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1730   53   77   40]\n",
            " [  26 1844   15   15]\n",
            " [  89   24 1627  160]\n",
            " [  88   19  122 1671]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds2, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds2.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds2 = torch.cat(Y_preds2)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds2.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds2 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds2, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds2 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds2)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds2, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds2))\n",
        "\n",
        "parameters2 = count_parameters(classifier)\n",
        "average_time_per_epoch2 = (end_time - start_time)/EPOCHS\n",
        "accuracy2 = accuracy_score(Y_actual, Y_preds2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34873cda-915f-4cd2-c2e8-ec1e10956612",
        "id": "VnjXOao7TDPQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2171996\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.947\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.858\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.844\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.826\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.821\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.818\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.815\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.830\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.819\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.814\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "\n",
            "Test Accuracy : 0.905\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.90      0.91      1900\n",
            "      Sports       0.93      0.98      0.95      1900\n",
            "    Business       0.87      0.88      0.87      1900\n",
            "    Sci/Tech       0.90      0.87      0.88      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1705   67   84   44]\n",
            " [  14 1856   21    9]\n",
            " [  69   30 1668  133]\n",
            " [  72   34  146 1648]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds3, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds3.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds3 = torch.cat(Y_preds3)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds3.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds3 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds3, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds3 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds3)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds3, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds3))\n",
        "\n",
        "parameters3 = count_parameters(classifier)\n",
        "average_time_per_epoch3 = (end_time - start_time)/EPOCHS\n",
        "accuracy3 = accuracy_score(Y_actual, Y_preds3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa099c2e-c25c-473a-aac6-f1e139febc4d",
        "id": "KFMUmOY6WjcP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2168156\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.033\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.858\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.841\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.824\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.818\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.811\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.806\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.804\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.802\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.800\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.799\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.798\n",
            "\n",
            "Test Accuracy : 0.908\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.93      0.90      0.91      1900\n",
            "      Sports       0.95      0.97      0.96      1900\n",
            "    Business       0.88      0.86      0.87      1900\n",
            "    Sci/Tech       0.87      0.90      0.89      1900\n",
            "\n",
            "    accuracy                           0.91      7600\n",
            "   macro avg       0.91      0.91      0.91      7600\n",
            "weighted avg       0.91      0.91      0.91      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1707   62   78   53]\n",
            " [  19 1843   15   23]\n",
            " [  60   23 1639  178]\n",
            " [  47   11  130 1712]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1 direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import time \n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds4, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds4.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds4 = torch.cat(Y_preds4)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds4.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds4 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds4, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds4 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds4)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds4, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds4))\n",
        "\n",
        "parameters4 = count_parameters(classifier)\n",
        "average_time_per_epoch4 = (end_time - start_time)/EPOCHS\n",
        "accuracy4 = accuracy_score(Y_actual, Y_preds4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds5, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds5.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds5 = torch.cat(Y_preds5)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds5.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds5 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds5, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds5 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds5)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds5, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds5))\n",
        "\n",
        "\n",
        "parameters5 = count_parameters(classifier)\n",
        "average_time_per_epoch5 = (end_time - start_time)/EPOCHS\n",
        "accuracy5 = accuracy_score(Y_actual, Y_preds5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOEk-PtaUS8",
        "outputId": "a0f6f3a4-b4b5-4a1f-ee69-cb1eca32aa37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2210908\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.986\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 16.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.849\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.826\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.820\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.806\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.803\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.801\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.799\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.797\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.796\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.794\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 16.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.794\n",
            "\n",
            "Test Accuracy : 0.912\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.93      0.90      0.91      1900\n",
            "      Sports       0.96      0.98      0.97      1900\n",
            "    Business       0.89      0.87      0.88      1900\n",
            "    Sci/Tech       0.88      0.90      0.89      1900\n",
            "\n",
            "    accuracy                           0.91      7600\n",
            "   macro avg       0.91      0.91      0.91      7600\n",
            "weighted avg       0.91      0.91      0.91      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1712   53   79   56]\n",
            " [  16 1858   14   12]\n",
            " [  63   20 1645  172]\n",
            " [  55   13  115 1717]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) ## Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds6, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds6.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds6 = torch.cat(Y_preds6)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds6.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds6 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds6, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds6 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds6)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds6, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds6))\n",
        "\n",
        "parameters6 = count_parameters(classifier)\n",
        "average_time_per_epoch6 = (end_time - start_time)/EPOCHS\n",
        "accuracy6 = accuracy_score(Y_actual, Y_preds6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36f447d-da49-4e71-e09e-cb5ce657b0a2",
        "id": "E3it53CzfhbY"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2310236\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:08<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.967\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.853\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.839\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.829\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.821\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.817\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.816\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.812\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.807\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.806\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.803\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.802\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.801\n",
            "\n",
            "Test Accuracy : 0.905\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.91      0.90      1900\n",
            "      Sports       0.96      0.96      0.96      1900\n",
            "    Business       0.88      0.87      0.87      1900\n",
            "    Sci/Tech       0.89      0.88      0.88      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1726   53   79   42]\n",
            " [  42 1825   19   14]\n",
            " [  80   13 1650  157]\n",
            " [  81    7  136 1676]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of text that were missclasified by all models and the most frequent pair of correct category and wrong prediction . The matrix of the accuracy, parameters and time cost per epoch of all the above models is below this block."
      ],
      "metadata": {
        "id": "v5feilg-j8U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = list(zip(Y_preds1, Y_preds2, Y_preds3, Y_preds4, Y_preds5, Y_preds6))\n",
        "\n",
        "\n",
        "misclassified_indices = []\n",
        "for i, (pred1, pred2, pred3, pred4, pred5, pred6) in enumerate(predictions):\n",
        "    if (pred1 != Y_actual[i]) and (pred2 != Y_actual[i]) and (pred3 != Y_actual[i]) and (pred4 != Y_actual[i]) and (pred5 != Y_actual[i]) and (pred6 != Y_actual[i]):\n",
        "        misclassified_indices.append(i)\n",
        "\n",
        "\n",
        "misclassified_counts = {category: 0 for category in np.unique(Y_actual)}\n",
        "\n",
        "\n",
        "pair_counts = {}\n",
        "\n",
        "for idx in misclassified_indices:\n",
        "    correct_category = Y_actual[idx]\n",
        "    misclassified_counts[correct_category] += 1\n",
        "\n",
        "    pair = (correct_category, Y_preds1[idx]) \n",
        "\n",
        "    if pair in pair_counts:\n",
        "        pair_counts[pair] += 1\n",
        "    else:\n",
        "        pair_counts[pair] = 1\n",
        "\n",
        "label_to_category = {\n",
        "    \"0\": \"0:World\",\n",
        "    \"1\": \"1:Sports\",\n",
        "    \"2\": \"2:Business\",\n",
        "    \"3\": \"3:Sci/Tech\",\n",
        "}\n",
        "\n",
        "\n",
        "misclassified_index = misclassified_indices[0]\n",
        "\n",
        "\n",
        "misclassified_text = test_dataset[misclassified_index][1]\n",
        "\n",
        "correct_category = Y_actual[misclassified_index]\n",
        "predicted_category = Y_preds1[misclassified_index]\n",
        "\n",
        "print(f\"Misclassified Text (Index: {misclassified_index}):\")\n",
        "print(misclassified_text)\n",
        "print(f\"\\nShould have been classified as: {label_to_category[str(correct_category)]}\")\n",
        "print(f\"\\nWas classified as: {label_to_category[str(predicted_category)]}\\n\")\n",
        "\n",
        "for category in sorted(misclassified_counts.keys()):\n",
        "    category_name = label_to_category[str(category)]\n",
        "    count = misclassified_counts[category]\n",
        "    print(f\"{category_name}: {count} samples\")\n",
        "\n",
        "\n",
        "most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "print(\"\\nThe most frequent pair of correct category and wrong prediction:\")\n",
        "print(f\"Correct category: {label_to_category[str(most_frequent_pair[0])]}, Wrong prediction: {label_to_category[str(most_frequent_pair[1])]}, Occurrences: {pair_counts[most_frequent_pair]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pppj9_OcvpxL",
        "outputId": "98a7f9bc-b260-4619-fb01-aa908467dabb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Text (Index: 56):\n",
            "India's Tata expands regional footprint via NatSteel buyout (AFP) AFP - India's Tata Iron and Steel Company Ltd. took a strategic step to expand its Asian footprint with the announcement it will buy the Asia-Pacific steel operations of Singapore's NatSteel Ltd.\n",
            "\n",
            "Should have been classified as: 0:World\n",
            "\n",
            "Was classified as: 2:Business\n",
            "\n",
            "0:World: 132 samples\n",
            "1:Sports: 21 samples\n",
            "2:Business: 158 samples\n",
            "3:Sci/Tech: 101 samples\n",
            "\n",
            "The most frequent pair of correct category and wrong prediction:\n",
            "Correct category: 2:Business, Wrong prediction: 3:Sci/Tech, Occurrences: 108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of accuracy, 1Bi-LSTM achieves the highest score at 0.912, closely followed by 2-Bi-RNN at 0.905 and 2Bi-LSTM at 0.905. The 1RNN has the lowest accuracy at 0.891, which is still relatively close to the other models.\n",
        "\n",
        "When using GloVe embeddings, it is evident that the models without pre-trained embeddings generally achieve lower accuracy. This comparison highlights the benefits of incorporating pre-trained embeddings like GloVe in natural language processing tasks. Pre-trained embeddings capture semantic and syntactic information, which can aid the models in making better predictions and result in higher accuracy."
      ],
      "metadata": {
        "id": "Mne99hwekAGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_values(value):\n",
        "    if isinstance(value, float):\n",
        "        if value.is_integer():\n",
        "            return f\"{int(value):,}\"\n",
        "        else:\n",
        "            return f\"{value:.3f}\"\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"1RNN\": [accuracy1, parameters1, average_time_per_epoch1],\n",
        "    \"1-Bi-RNN\": [accuracy2, parameters2, average_time_per_epoch2],\n",
        "    \"2-Bi-RNN\": [accuracy3, parameters3, average_time_per_epoch3],\n",
        "    \"1LSTM\": [accuracy4, parameters4, average_time_per_epoch4],\n",
        "    \"1Bi-LSTM\": [accuracy5, parameters5, average_time_per_epoch5],\n",
        "    \"2Bi-LSTM\": [accuracy6, parameters6, average_time_per_epoch6],\n",
        "}\n",
        "\n",
        "index = [\"Accuracy (%)\", \"Parameters\", \"Time cost per epoch (s)\"]\n",
        "\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "df = df.applymap(format_values)\n",
        "\n",
        "df = df.style \\\n",
        "    .set_properties(**{'font-weight': 'bold', 'border': '2px solid black'}) \\\n",
        "    .set_table_styles([dict(selector='th', props=[('font-weight', 'bold'), ('border', '1px solid black')])])\n",
        "\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "_eZiZOZcLABh",
        "outputId": "063440cf-74d2-47cd-b7fd-7648342da7aa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc1c004bac0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_7fe9f th {\n",
              "  font-weight: bold;\n",
              "  border: 1px solid black;\n",
              "}\n",
              "#T_7fe9f_row0_col0, #T_7fe9f_row0_col1, #T_7fe9f_row0_col2, #T_7fe9f_row0_col3, #T_7fe9f_row0_col4, #T_7fe9f_row0_col5, #T_7fe9f_row1_col0, #T_7fe9f_row1_col1, #T_7fe9f_row1_col2, #T_7fe9f_row1_col3, #T_7fe9f_row1_col4, #T_7fe9f_row1_col5, #T_7fe9f_row2_col0, #T_7fe9f_row2_col1, #T_7fe9f_row2_col2, #T_7fe9f_row2_col3, #T_7fe9f_row2_col4, #T_7fe9f_row2_col5 {\n",
              "  font-weight: bold;\n",
              "  border: 2px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_7fe9f\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_7fe9f_level0_col0\" class=\"col_heading level0 col0\" >1RNN</th>\n",
              "      <th id=\"T_7fe9f_level0_col1\" class=\"col_heading level0 col1\" >1-Bi-RNN</th>\n",
              "      <th id=\"T_7fe9f_level0_col2\" class=\"col_heading level0 col2\" >2-Bi-RNN</th>\n",
              "      <th id=\"T_7fe9f_level0_col3\" class=\"col_heading level0 col3\" >1LSTM</th>\n",
              "      <th id=\"T_7fe9f_level0_col4\" class=\"col_heading level0 col4\" >1Bi-LSTM</th>\n",
              "      <th id=\"T_7fe9f_level0_col5\" class=\"col_heading level0 col5\" >2Bi-LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_7fe9f_level0_row0\" class=\"row_heading level0 row0\" >Accuracy (%)</th>\n",
              "      <td id=\"T_7fe9f_row0_col0\" class=\"data row0 col0\" >0.891</td>\n",
              "      <td id=\"T_7fe9f_row0_col1\" class=\"data row0 col1\" >0.904</td>\n",
              "      <td id=\"T_7fe9f_row0_col2\" class=\"data row0 col2\" >0.905</td>\n",
              "      <td id=\"T_7fe9f_row0_col3\" class=\"data row0 col3\" >0.908</td>\n",
              "      <td id=\"T_7fe9f_row0_col4\" class=\"data row0 col4\" >0.912</td>\n",
              "      <td id=\"T_7fe9f_row0_col5\" class=\"data row0 col5\" >0.905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7fe9f_level0_row1\" class=\"row_heading level0 row1\" >Parameters</th>\n",
              "      <td id=\"T_7fe9f_row1_col0\" class=\"data row1 col0\" >2,136,284</td>\n",
              "      <td id=\"T_7fe9f_row1_col1\" class=\"data row1 col1\" >2,147,164</td>\n",
              "      <td id=\"T_7fe9f_row1_col2\" class=\"data row1 col2\" >2,171,996</td>\n",
              "      <td id=\"T_7fe9f_row1_col3\" class=\"data row1 col3\" >2,168,156</td>\n",
              "      <td id=\"T_7fe9f_row1_col4\" class=\"data row1 col4\" >2,210,908</td>\n",
              "      <td id=\"T_7fe9f_row1_col5\" class=\"data row1 col5\" >2,310,236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7fe9f_level0_row2\" class=\"row_heading level0 row2\" >Time cost per epoch (s)</th>\n",
              "      <td id=\"T_7fe9f_row2_col0\" class=\"data row2 col0\" >5.033</td>\n",
              "      <td id=\"T_7fe9f_row2_col1\" class=\"data row2 col1\" >5.267</td>\n",
              "      <td id=\"T_7fe9f_row2_col2\" class=\"data row2 col2\" >5.763</td>\n",
              "      <td id=\"T_7fe9f_row2_col3\" class=\"data row2 col3\" >5.485</td>\n",
              "      <td id=\"T_7fe9f_row2_col4\" class=\"data row2 col4\" >6.301</td>\n",
              "      <td id=\"T_7fe9f_row2_col5\" class=\"data row2 col5\" >7.370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}