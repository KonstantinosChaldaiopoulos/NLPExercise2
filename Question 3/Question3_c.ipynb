{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuPII49Y5pCM",
        "outputId": "f2e3780b-574a-4df9-f924-70e1275858a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "zxdLGlySOxZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file is basically the same as the Question3_a-b with the only difference being the MAX_WORDS which is set to 50 in all the models respectively:"
      ],
      "metadata": {
        "id": "s-CAB3Q8Kg-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main block of an 1-direction RNN classifier.This code was provided for all the other blocks to be based on:"
      ],
      "metadata": {
        "id": "1qZ-VsyUJ8nO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1qRxgRtEZ9B",
        "outputId": "cad3c638-7cdf-4099-fc5d-6f7df69fc612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.375\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.323\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.323\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.311\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.320\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.291\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.275\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.265\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.271\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.268\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.262\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.247\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.270\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.316\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.328\n",
            "\n",
            "Test Accuracy : 0.377\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.63      0.47      0.54      1900\n",
            "      Sports       0.31      0.84      0.45      1900\n",
            "    Business       0.36      0.10      0.16      1900\n",
            "    Sci/Tech       0.35      0.10      0.15      1900\n",
            "\n",
            "    accuracy                           0.38      7600\n",
            "   macro avg       0.41      0.38      0.32      7600\n",
            "weighted avg       0.41      0.38      0.32      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[ 892  798  113   97]\n",
            " [ 107 1599   91  103]\n",
            " [ 277 1298  190  135]\n",
            " [ 130 1456  133  181]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 50\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds1, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds1.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds1 = torch.cat(Y_preds1)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds1.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds1 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds1, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds1 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds1)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds1, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds1))\n",
        "\n",
        "parameters1 = count_parameters(classifier)\n",
        "average_time_per_epoch1 = (end_time - start_time)/EPOCHS\n",
        "accuracy1 = accuracy_score(Y_actual, Y_preds1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction RNN classifier. The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "99SlYcJXKu7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac75602-5481-4fd9-d1f2-2a01667d316b",
        "id": "eXjKHZ-4OyDQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2147164\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.263\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.011\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.934\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.902\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.884\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.868\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.864\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.854\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.846\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.844\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.836\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.830\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.827\n",
            "\n",
            "Test Accuracy : 0.882\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.89      0.89      1900\n",
            "      Sports       0.94      0.94      0.94      1900\n",
            "    Business       0.85      0.83      0.84      1900\n",
            "    Sci/Tech       0.85      0.86      0.86      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1690   65   76   69]\n",
            " [  28 1795   49   28]\n",
            " [ 108   26 1573  193]\n",
            " [  81   28  149 1642]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 50\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds2, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds2.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds2 = torch.cat(Y_preds2)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds2.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds2 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds2, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds2 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds2)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds2, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds2))\n",
        "\n",
        "parameters2 = count_parameters(classifier)\n",
        "average_time_per_epoch2 = (end_time - start_time)/EPOCHS\n",
        "accuracy2 = accuracy_score(Y_actual, Y_preds2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction RNN classifier with 2 layers.The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "NE83WRnfK4PV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2183fa-6822-413d-dcff-9172ecd29505",
        "id": "VnjXOao7TDPQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2171996\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.235\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.026\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.959\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.928\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.905\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.891\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.892\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.874\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.869\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.865\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 16.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.868\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.880\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.862\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.848\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 16.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.846\n",
            "\n",
            "Test Accuracy : 0.881\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.88      0.88      0.88      1900\n",
            "      Sports       0.93      0.96      0.94      1900\n",
            "    Business       0.86      0.84      0.85      1900\n",
            "    Sci/Tech       0.86      0.84      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1680   68  100   52]\n",
            " [  41 1825   21   13]\n",
            " [  73   32 1594  201]\n",
            " [ 111   41  148 1600]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 50\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds3, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds3.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds3 = torch.cat(Y_preds3)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds3.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds3 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds3, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds3 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds3)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds3, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds3))\n",
        "\n",
        "parameters3 = count_parameters(classifier)\n",
        "average_time_per_epoch3 = (end_time - start_time)/EPOCHS\n",
        "accuracy3 = accuracy_score(Y_actual, Y_preds3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a one-direction LSTM classifier.The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "f_OIiX2EK6dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e89156-40c8-41f2-a1fb-e01707411086",
        "id": "KFMUmOY6WjcP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2168156\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.339\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.072\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.948\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.908\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.889\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.877\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.873\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.861\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.854\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.851\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.848\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.857\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.846\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "\n",
            "Test Accuracy : 0.885\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.87      0.89      1900\n",
            "      Sports       0.93      0.95      0.94      1900\n",
            "    Business       0.84      0.85      0.85      1900\n",
            "    Sci/Tech       0.85      0.87      0.86      1900\n",
            "\n",
            "    accuracy                           0.89      7600\n",
            "   macro avg       0.89      0.89      0.89      7600\n",
            "weighted avg       0.89      0.89      0.89      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1653   71  116   60]\n",
            " [  33 1806   16   45]\n",
            " [  71   24 1614  191]\n",
            " [  42   34  171 1653]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1 direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import time \n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 50\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds4, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds4.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds4 = torch.cat(Y_preds4)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds4.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds4 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds4, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds4 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds4)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds4, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds4))\n",
        "\n",
        "parameters4 = count_parameters(classifier)\n",
        "average_time_per_epoch4 = (end_time - start_time)/EPOCHS\n",
        "accuracy4 = accuracy_score(Y_actual, Y_preds4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction LSTM classifier. The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "YATMx_i5LGHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 50\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds5, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds5.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds5 = torch.cat(Y_preds5)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds5.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds5 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds5, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds5 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds5)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds5, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds5))\n",
        "\n",
        "\n",
        "parameters5 = count_parameters(classifier)\n",
        "average_time_per_epoch5 = (end_time - start_time)/EPOCHS\n",
        "accuracy5 = accuracy_score(Y_actual, Y_preds5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOEk-PtaUS8",
        "outputId": "147bea84-8b8e-47ba-96a5-959febeacacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2210908\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.220\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.933\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.878\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.855\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.841\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.824\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.818\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.811\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.806\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.803\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.800\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.800\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.797\n",
            "\n",
            "Test Accuracy : 0.899\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.93      0.89      0.91      1900\n",
            "      Sports       0.94      0.97      0.95      1900\n",
            "    Business       0.88      0.84      0.86      1900\n",
            "    Sci/Tech       0.85      0.89      0.87      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1691   61   92   56]\n",
            " [  19 1851   12   18]\n",
            " [  55   30 1595  220]\n",
            " [  49   36  123 1692]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bi-direction LSTM classifier with 2 layers. The changes in the code were the following in the \"model\" class:\n",
        "\n",
        "\n",
        "```\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) ## Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gONag97LLLb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 50\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) ## Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds6, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds6.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds6 = torch.cat(Y_preds6)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds6.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds6 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds6, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds6 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds6)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds6, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds6))\n",
        "\n",
        "parameters6 = count_parameters(classifier)\n",
        "average_time_per_epoch6 = (end_time - start_time)/EPOCHS\n",
        "accuracy6 = accuracy_score(Y_actual, Y_preds6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3215ff63-1567-4a4c-a522-05ac4fa33f13",
        "id": "E3it53CzfhbY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2310236\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.170\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.924\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.876\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.857\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 16.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.833\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.828\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.822\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.816\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.813\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.809\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.807\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.805\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:07<00:00, 14.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.805\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.802\n",
            "\n",
            "Test Accuracy : 0.895\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.93      0.87      0.90      1900\n",
            "      Sports       0.96      0.94      0.95      1900\n",
            "    Business       0.87      0.86      0.86      1900\n",
            "    Sci/Tech       0.84      0.91      0.87      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1649   57   99   95]\n",
            " [  43 1788   30   39]\n",
            " [  47   16 1635  202]\n",
            " [  36   11  123 1730]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of text that were missclasified by all models and the most frequent pair of correct category and wrong prediction . The matrix of the accuracy, parameters and time cost per epoch of all the above models is below this block."
      ],
      "metadata": {
        "id": "OZnK4WB0LTjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = list(zip(Y_preds1, Y_preds2, Y_preds3, Y_preds4, Y_preds5, Y_preds6))\n",
        "\n",
        "\n",
        "misclassified_indices = []\n",
        "for i, (pred1, pred2, pred3, pred4, pred5, pred6) in enumerate(predictions):\n",
        "    if (pred1 != Y_actual[i]) and (pred2 != Y_actual[i]) and (pred3 != Y_actual[i]) and (pred4 != Y_actual[i]) and (pred5 != Y_actual[i]) and (pred6 != Y_actual[i]):\n",
        "        misclassified_indices.append(i)\n",
        "\n",
        "\n",
        "misclassified_counts = {category: 0 for category in np.unique(Y_actual)}\n",
        "\n",
        "\n",
        "pair_counts = {}\n",
        "\n",
        "for idx in misclassified_indices:\n",
        "    correct_category = Y_actual[idx]\n",
        "    misclassified_counts[correct_category] += 1\n",
        "\n",
        "    pair = (correct_category, Y_preds1[idx])  \n",
        "\n",
        "    if pair in pair_counts:\n",
        "        pair_counts[pair] += 1\n",
        "    else:\n",
        "        pair_counts[pair] = 1\n",
        "\n",
        "label_to_category = {\n",
        "    \"0\": \"0:World\",\n",
        "    \"1\": \"1:Sports\",\n",
        "    \"2\": \"2:Business\",\n",
        "    \"3\": \"3:Sci/Tech\",\n",
        "}\n",
        "\n",
        "\n",
        "misclassified_index = misclassified_indices[0]\n",
        "\n",
        "\n",
        "misclassified_text = test_dataset[misclassified_index][1]\n",
        "\n",
        "\n",
        "correct_category = Y_actual[misclassified_index]\n",
        "predicted_category = Y_preds1[misclassified_index]\n",
        "\n",
        "\n",
        "print(f\"Misclassified Text (Index: {misclassified_index}):\")\n",
        "print(misclassified_text)\n",
        "print(f\"\\nShould have been classified as: {label_to_category[str(correct_category)]}\")\n",
        "print(f\"\\nWas classified as: {label_to_category[str(predicted_category)]}\\n\")\n",
        "\n",
        "for category in sorted(misclassified_counts.keys()):\n",
        "    category_name = label_to_category[str(category)]\n",
        "    count = misclassified_counts[category]\n",
        "    print(f\"{category_name}: {count} samples\")\n",
        "\n",
        "\n",
        "most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "print(\"\\nThe most frequent pair of correct category and wrong prediction:\")\n",
        "print(f\"Correct category: {label_to_category[str(most_frequent_pair[0])]}, Wrong prediction: {label_to_category[str(most_frequent_pair[1])]}, Occurrences: {pair_counts[most_frequent_pair]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pppj9_OcvpxL",
        "outputId": "650f74f2-5474-4509-ecab-fff772a46f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Text (Index: 9):\n",
            "Card fraud unit nets 36,000 cards In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\n",
            "\n",
            "Should have been classified as: 3:Sci/Tech\n",
            "\n",
            "Was classified as: 1:Sports\n",
            "\n",
            "0:World: 94 samples\n",
            "1:Sports: 1 samples\n",
            "2:Business: 107 samples\n",
            "3:Sci/Tech: 66 samples\n",
            "\n",
            "The most frequent pair of correct category and wrong prediction:\n",
            "Correct category: 2:Business, Wrong prediction: 1:Sports, Occurrences: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix of the accuracy, parameters and time cost per epoch of all the above models with MAX_WORDS set to 50. As we can see in the results there is a minor increase in accuracy for all models except from the 1 directional RNN which has its accuracy severely decreased( 0.868 to 0.377 ). The increase in accuracy is most significant in the the bi-directional LSTM with 1 layer and the highest accuracy is also achieved in the bi-directional LSTM with 1 layer( 0.890 to 0.899 ). There is a significant increase in time ~0.5-1.2 sec per epoch for each model from the 25 MAX_WORDS counterpart. Like before, the time cost per epoch increases as we progress from simpler to more complex architectures."
      ],
      "metadata": {
        "id": "HffdKYNELdwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_values(value):\n",
        "    if isinstance(value, float):\n",
        "        if value.is_integer():\n",
        "            return f\"{int(value):,}\"\n",
        "        else:\n",
        "            return f\"{value:.3f}\"\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"1RNN\": [accuracy1, parameters1, average_time_per_epoch1],\n",
        "    \"1-Bi-RNN\": [accuracy2, parameters2, average_time_per_epoch2],\n",
        "    \"2-Bi-RNN\": [accuracy3, parameters3, average_time_per_epoch3],\n",
        "    \"1LSTM\": [accuracy4, parameters4, average_time_per_epoch4],\n",
        "    \"1Bi-LSTM\": [accuracy5, parameters5, average_time_per_epoch5],\n",
        "    \"2Bi-LSTM\": [accuracy6, parameters6, average_time_per_epoch6],\n",
        "}\n",
        "\n",
        "index = [\"Accuracy (%)\", \"Parameters\", \"Time cost per epoch (s)\"]\n",
        "\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "df = df.applymap(format_values)\n",
        "\n",
        "df = df.style \\\n",
        "    .set_properties(**{'font-weight': 'bold', 'border': '2px solid black'}) \\\n",
        "    .set_table_styles([dict(selector='th', props=[('font-weight', 'bold'), ('border', '1px solid black')])])\n",
        "\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "_eZiZOZcLABh",
        "outputId": "f491bd4f-643e-4a74-f2cf-7f8e63bcc6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fcc536cbdf0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_48724 th {\n",
              "  font-weight: bold;\n",
              "  border: 1px solid black;\n",
              "}\n",
              "#T_48724_row0_col0, #T_48724_row0_col1, #T_48724_row0_col2, #T_48724_row0_col3, #T_48724_row0_col4, #T_48724_row0_col5, #T_48724_row1_col0, #T_48724_row1_col1, #T_48724_row1_col2, #T_48724_row1_col3, #T_48724_row1_col4, #T_48724_row1_col5, #T_48724_row2_col0, #T_48724_row2_col1, #T_48724_row2_col2, #T_48724_row2_col3, #T_48724_row2_col4, #T_48724_row2_col5 {\n",
              "  font-weight: bold;\n",
              "  border: 2px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_48724\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_48724_level0_col0\" class=\"col_heading level0 col0\" >1RNN</th>\n",
              "      <th id=\"T_48724_level0_col1\" class=\"col_heading level0 col1\" >1-Bi-RNN</th>\n",
              "      <th id=\"T_48724_level0_col2\" class=\"col_heading level0 col2\" >2-Bi-RNN</th>\n",
              "      <th id=\"T_48724_level0_col3\" class=\"col_heading level0 col3\" >1LSTM</th>\n",
              "      <th id=\"T_48724_level0_col4\" class=\"col_heading level0 col4\" >1Bi-LSTM</th>\n",
              "      <th id=\"T_48724_level0_col5\" class=\"col_heading level0 col5\" >2Bi-LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_48724_level0_row0\" class=\"row_heading level0 row0\" >Accuracy (%)</th>\n",
              "      <td id=\"T_48724_row0_col0\" class=\"data row0 col0\" >0.377</td>\n",
              "      <td id=\"T_48724_row0_col1\" class=\"data row0 col1\" >0.882</td>\n",
              "      <td id=\"T_48724_row0_col2\" class=\"data row0 col2\" >0.881</td>\n",
              "      <td id=\"T_48724_row0_col3\" class=\"data row0 col3\" >0.885</td>\n",
              "      <td id=\"T_48724_row0_col4\" class=\"data row0 col4\" >0.899</td>\n",
              "      <td id=\"T_48724_row0_col5\" class=\"data row0 col5\" >0.895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_48724_level0_row1\" class=\"row_heading level0 row1\" >Parameters</th>\n",
              "      <td id=\"T_48724_row1_col0\" class=\"data row1 col0\" >2,136,284</td>\n",
              "      <td id=\"T_48724_row1_col1\" class=\"data row1 col1\" >2,147,164</td>\n",
              "      <td id=\"T_48724_row1_col2\" class=\"data row1 col2\" >2,171,996</td>\n",
              "      <td id=\"T_48724_row1_col3\" class=\"data row1 col3\" >2,168,156</td>\n",
              "      <td id=\"T_48724_row1_col4\" class=\"data row1 col4\" >2,210,908</td>\n",
              "      <td id=\"T_48724_row1_col5\" class=\"data row1 col5\" >2,310,236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_48724_level0_row2\" class=\"row_heading level0 row2\" >Time cost per epoch (s)</th>\n",
              "      <td id=\"T_48724_row2_col0\" class=\"data row2 col0\" >5.167</td>\n",
              "      <td id=\"T_48724_row2_col1\" class=\"data row2 col1\" >5.621</td>\n",
              "      <td id=\"T_48724_row2_col2\" class=\"data row2 col2\" >6.445</td>\n",
              "      <td id=\"T_48724_row2_col3\" class=\"data row2 col3\" >6.075</td>\n",
              "      <td id=\"T_48724_row2_col4\" class=\"data row2 col4\" >7.080</td>\n",
              "      <td id=\"T_48724_row2_col5\" class=\"data row2 col5\" >7.299</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}