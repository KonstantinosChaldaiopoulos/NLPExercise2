{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuPII49Y5pCM",
        "outputId": "3f7ed2b7-8591-4c70-d460-65cadc72543d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the file whose main focus is to use the pre-trained GloVe embeddings while freezing them.\n",
        "\n"
      ],
      "metadata": {
        "id": "M20iU5v4CDg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.vocab import GloVe\n"
      ],
      "metadata": {
        "id": "zxdLGlySOxZ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initialization of the embedding layer using pre-trained GloVe embeddings for the words in the vocabulary.  For words not found in the GloVe embeddings, it assigns them random embeddings within a specified range. The function also prints the number of words for which pre-trained embeddings were found out of the total words in the vocabulary. From the previous files, there is only the bellow differences. If one wants to see the differences in the construction of the different models, one should consult the previous files. "
      ],
      "metadata": {
        "id": "UcdUj0HZCKT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_embedding_layer(embedding_layer, glove_embeddings, vocab):\n",
        "    pretrained_weights = torch.zeros((len(vocab), EMBEDDING_DIM))\n",
        "    found_words = 0\n",
        "    for idx, word in enumerate(vocab.get_itos()):\n",
        "        if word in glove_embeddings.stoi:\n",
        "            pretrained_weights[idx] = glove_embeddings[word]\n",
        "            found_words += 1\n",
        "        else:\n",
        "            pretrained_weights[idx] = torch.FloatTensor(EMBEDDING_DIM).uniform_(-0.6, 0.6)\n",
        "\n",
        "    embedding_layer.weight.data.copy_(pretrained_weights)\n",
        "    print(f'Found {found_words} words with pre-trained embeddings out of {len(vocab)} total words.')\n"
      ],
      "metadata": {
        "id": "UZABF1Wo3wqc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In every model the following changes were made. Notice the freezing of the layer by using this line.\n",
        "\n",
        "```\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code = Freezing the embedding layer\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        ".\n",
        ".\n",
        ".\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM) # Change in code - Creates a glove object using the GloVe pre-trained word embeddings.\n",
        ".\n",
        ".\n",
        ".\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab) # Change in code - Call of the initialize_embedding_layer function withe the glove object.\n",
        "classifier.embedding_layer.weight.requires_grad = False `# Change in code- Freezing the embedding layer` \n",
        ".\n",
        ".\n",
        ".\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tmI46kFNCQaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1qRxgRtEZ9B",
        "outputId": "7f4d4518-13ce-4a65-c504-fc6b125be7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.38MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:17<00:00, 23432.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  10884\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.068\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.901\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.901\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.888\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.883\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.890\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.886\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.884\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.880\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.879\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.881\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.883\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.880\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.879\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 31.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.876\n",
            "\n",
            "Test Accuracy : 0.868\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.86      0.88      1900\n",
            "      Sports       0.94      0.95      0.94      1900\n",
            "    Business       0.84      0.80      0.82      1900\n",
            "    Sci/Tech       0.81      0.86      0.84      1900\n",
            "\n",
            "    accuracy                           0.87      7600\n",
            "   macro avg       0.87      0.87      0.87      7600\n",
            "weighted avg       0.87      0.87      0.87      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1634   67  104   95]\n",
            " [  41 1805   20   34]\n",
            " [  89   36 1524  251]\n",
            " [  69   22  172 1637]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code- Freezing the embedding layer\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds1, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds1.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds1 = torch.cat(Y_preds1)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds1.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds1 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds1, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds1 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds1)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds1, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds1))\n",
        "\n",
        "parameters1 = count_parameters(classifier)\n",
        "average_time_per_epoch1 = (end_time - start_time)/EPOCHS\n",
        "accuracy1 = accuracy_score(Y_actual, Y_preds1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ee9822-9325-4ea6-82a5-7cda5c70ced5",
        "id": "eXjKHZ-4OyDQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  21764\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 29.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.996\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.881\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.874\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.869\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.867\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.866\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.865\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.863\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.863\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 29.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.860\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.861\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.861\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 30.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.857\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 29.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.858\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.857\n",
            "\n",
            "Test Accuracy : 0.879\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.94      0.84      0.89      1900\n",
            "      Sports       0.93      0.97      0.95      1900\n",
            "    Business       0.84      0.82      0.83      1900\n",
            "    Sci/Tech       0.82      0.89      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1603   77  125   95]\n",
            " [   9 1841   22   28]\n",
            " [  54   41 1553  252]\n",
            " [  43   27  145 1685]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code- Freezing the embedding layer\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds2, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds2.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds2 = torch.cat(Y_preds2)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds2.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds2 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds2, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds2 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds2)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds2, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds2))\n",
        "\n",
        "parameters2 = count_parameters(classifier)\n",
        "average_time_per_epoch2 = (end_time - start_time)/EPOCHS\n",
        "accuracy2 = accuracy_score(Y_actual, Y_preds2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e46fcd0-3f1a-4349-dde1-da8f8846adba",
        "id": "VnjXOao7TDPQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  46596\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.958\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.878\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.876\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.871\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.870\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.869\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.869\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.871\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.865\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.865\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.873\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 27.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.868\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.885\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.887\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.896\n",
            "\n",
            "Test Accuracy : 0.862\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.87      0.86      0.87      1900\n",
            "      Sports       0.96      0.92      0.94      1900\n",
            "    Business       0.84      0.78      0.81      1900\n",
            "    Sci/Tech       0.79      0.89      0.84      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1641   53  101  105]\n",
            " [  45 1739   51   65]\n",
            " [ 124   15 1480  281]\n",
            " [  77   12  123 1688]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code- Freezing the embedding layer\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds3, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds3.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds3 = torch.cat(Y_preds3)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds3.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds3 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds3, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds3 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds3)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds3, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds3))\n",
        "\n",
        "parameters3 = count_parameters(classifier)\n",
        "average_time_per_epoch3 = (end_time - start_time)/EPOCHS\n",
        "accuracy3 = accuracy_score(Y_actual, Y_preds3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94c047f-d62e-4054-aae0-bcd3e1ee8a5c",
        "id": "KFMUmOY6WjcP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  42756\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 1.034\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.874\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.864\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.858\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.853\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.851\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.846\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.845\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 28.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.841\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.838\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 29.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.835\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.834\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "\n",
            "Test Accuracy : 0.894\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.90      0.90      1900\n",
            "      Sports       0.95      0.96      0.96      1900\n",
            "    Business       0.89      0.81      0.85      1900\n",
            "    Sci/Tech       0.85      0.90      0.87      1900\n",
            "\n",
            "    accuracy                           0.89      7600\n",
            "   macro avg       0.89      0.89      0.89      7600\n",
            "weighted avg       0.89      0.89      0.89      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1718   55   63   64]\n",
            " [  34 1831   17   18]\n",
            " [ 109   24 1539  228]\n",
            " [  66   15  114 1705]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1 direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import time \n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code- Freezing the embedding layer\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds4, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds4.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds4 = torch.cat(Y_preds4)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds4.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds4 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds4, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds4 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds4)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds4, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds4))\n",
        "\n",
        "parameters4 = count_parameters(classifier)\n",
        "average_time_per_epoch4 = (end_time - start_time)/EPOCHS\n",
        "accuracy4 = accuracy_score(Y_actual, Y_preds4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code- Freezing the embedding layer\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds5, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds5.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds5 = torch.cat(Y_preds5)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds5.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds5 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds5, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds5 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds5)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds5, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds5))\n",
        "\n",
        "\n",
        "parameters5 = count_parameters(classifier)\n",
        "average_time_per_epoch5 = (end_time - start_time)/EPOCHS\n",
        "accuracy5 = accuracy_score(Y_actual, Y_preds5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOEk-PtaUS8",
        "outputId": "bc748c6c-b3c3-40b3-bb0a-3aaebd45aa32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  85508\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.994\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.868\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.860\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.855\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.852\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.848\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.845\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.842\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.841\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.836\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 20.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.832\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 26.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.831\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 21.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.828\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.826\n",
            "\n",
            "Test Accuracy : 0.902\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.91      0.90      0.91      1900\n",
            "      Sports       0.95      0.97      0.96      1900\n",
            "    Business       0.89      0.84      0.86      1900\n",
            "    Sci/Tech       0.85      0.90      0.88      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1712   59   73   56]\n",
            " [  20 1839   18   23]\n",
            " [  81   17 1591  211]\n",
            " [  69   13  108 1710]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier with 2 layers applied to AG_NEWS dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
        "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) ## Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "initialize_embedding_layer(classifier.embedding_layer, glove, vocab)\n",
        "classifier.embedding_layer.weight.requires_grad = False # Change in code- Freezing the embedding layer\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds6, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds6.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds6 = torch.cat(Y_preds6)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds6.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds6 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds6, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds6 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds6)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds6, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds6))\n",
        "\n",
        "parameters6 = count_parameters(classifier)\n",
        "average_time_per_epoch6 = (end_time - start_time)/EPOCHS\n",
        "accuracy6 = accuracy_score(Y_actual, Y_preds6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f90eb2-6e60-4dd4-9722-3263748cbcb0",
        "id": "E3it53CzfhbY"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20435 words with pre-trained embeddings out of 21254 total words.\n",
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  184836\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.975\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.873\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.865\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.861\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.857\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.855\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.852\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.849\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 23.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.848\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 25.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.845\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.843\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.842\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.841\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.837\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.838\n",
            "\n",
            "Test Accuracy : 0.899\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.93      0.88      0.91      1900\n",
            "      Sports       0.94      0.97      0.96      1900\n",
            "    Business       0.87      0.84      0.86      1900\n",
            "    Sci/Tech       0.85      0.90      0.87      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1681   64   92   63]\n",
            " [  16 1852   16   16]\n",
            " [  57   26 1596  221]\n",
            " [  48   18  128 1706]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of text that were missclasified by all models and the most frequent pair of correct category and wrong prediction . The matrix of the accuracy, parameters and time cost per epoch of all the above models is below this block."
      ],
      "metadata": {
        "id": "IA3wowSjEV5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = list(zip(Y_preds1, Y_preds2, Y_preds3, Y_preds4, Y_preds5, Y_preds6))\n",
        "\n",
        "\n",
        "misclassified_indices = []\n",
        "for i, (pred1, pred2, pred3, pred4, pred5, pred6) in enumerate(predictions):\n",
        "    if (pred1 != Y_actual[i]) and (pred2 != Y_actual[i]) and (pred3 != Y_actual[i]) and (pred4 != Y_actual[i]) and (pred5 != Y_actual[i]) and (pred6 != Y_actual[i]):\n",
        "        misclassified_indices.append(i)\n",
        "\n",
        "misclassified_counts = {category: 0 for category in np.unique(Y_actual)}\n",
        "\n",
        "\n",
        "pair_counts = {}\n",
        "\n",
        "for idx in misclassified_indices:\n",
        "    correct_category = Y_actual[idx]\n",
        "    misclassified_counts[correct_category] += 1\n",
        "\n",
        "    pair = (correct_category, Y_preds1[idx])  \n",
        "\n",
        "    if pair in pair_counts:\n",
        "        pair_counts[pair] += 1\n",
        "    else:\n",
        "        pair_counts[pair] = 1\n",
        "\n",
        "label_to_category = {\n",
        "    \"0\": \"0:World\",\n",
        "    \"1\": \"1:Sports\",\n",
        "    \"2\": \"2:Business\",\n",
        "    \"3\": \"3:Sci/Tech\",\n",
        "}\n",
        "\n",
        "\n",
        "misclassified_index = misclassified_indices[0]\n",
        "\n",
        "\n",
        "misclassified_text = test_dataset[misclassified_index][1]\n",
        "\n",
        "\n",
        "correct_category = Y_actual[misclassified_index]\n",
        "predicted_category = Y_preds1[misclassified_index]\n",
        "\n",
        "\n",
        "print(f\"Misclassified Text (Index: {misclassified_index}):\")\n",
        "print(misclassified_text)\n",
        "print(f\"\\nShould have been classified as: {label_to_category[str(correct_category)]}\")\n",
        "print(f\"\\nWas classified as: {label_to_category[str(predicted_category)]}\\n\")\n",
        "\n",
        "for category in sorted(misclassified_counts.keys()):\n",
        "    category_name = label_to_category[str(category)]\n",
        "    count = misclassified_counts[category]\n",
        "    print(f\"{category_name}: {count} samples\")\n",
        "\n",
        "most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "print(\"\\nThe most frequent pair of correct category and wrong prediction:\")\n",
        "print(f\"Correct category: {label_to_category[str(most_frequent_pair[0])]}, Wrong prediction: {label_to_category[str(most_frequent_pair[1])]}, Occurrences: {pair_counts[most_frequent_pair]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pppj9_OcvpxL",
        "outputId": "a10edc03-e4b4-43b2-d304-540138fa08ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Text (Index: 79):\n",
            "Live: Olympics day four Richard Faulds and Stephen Parry are going for gold for Great Britain on day four in Athens.\n",
            "\n",
            "Should have been classified as: 0:World\n",
            "\n",
            "Was classified as: 1:Sports\n",
            "\n",
            "0:World: 150 samples\n",
            "1:Sports: 32 samples\n",
            "2:Business: 186 samples\n",
            "3:Sci/Tech: 92 samples\n",
            "\n",
            "The most frequent pair of correct category and wrong prediction:\n",
            "Correct category: 2:Business, Wrong prediction: 3:Sci/Tech, Occurrences: 132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the layers are not frozen, the accuracies range from 0.891 (1RNN) to 0.912 (1Bi-LSTM). In contrast, when the layers are frozen, the accuracies are generally lower, ranging from 0.862 (2-Bi-RNN) to 0.902 (1Bi-LSTM). Freezing the layers prevents certain weights from being updated during training, which may result in suboptimal performance. Another notable difference is the number of trainable parameters in the models. The models with frozen layers need to update significantly fewer parameters compared to their counterparts without frozen layers. This is because freezing layers reduces the number of trainable parameters, leading to a more lightweight model with lower computational requirements. The time cost per epoch is also slightly different between the two cases. In general, models with frozen layers take less time per epoch, likely due to the reduced number of parameters that need to be updated during training."
      ],
      "metadata": {
        "id": "vJ0plojIDlJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_values(value):\n",
        "    if isinstance(value, float):\n",
        "        if value.is_integer():\n",
        "            return f\"{int(value):,}\"\n",
        "        else:\n",
        "            return f\"{value:.3f}\"\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"1RNN\": [accuracy1, parameters1, average_time_per_epoch1],\n",
        "    \"1-Bi-RNN\": [accuracy2, parameters2, average_time_per_epoch2],\n",
        "    \"2-Bi-RNN\": [accuracy3, parameters3, average_time_per_epoch3],\n",
        "    \"1LSTM\": [accuracy4, parameters4, average_time_per_epoch4],\n",
        "    \"1Bi-LSTM\": [accuracy5, parameters5, average_time_per_epoch5],\n",
        "    \"2Bi-LSTM\": [accuracy6, parameters6, average_time_per_epoch6],\n",
        "}\n",
        "\n",
        "index = [\"Accuracy (%)\", \"Parameters\", \"Time cost per epoch (s)\"]\n",
        "\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "df = df.applymap(format_values)\n",
        "\n",
        "df = df.style \\\n",
        "    .set_properties(**{'font-weight': 'bold', 'border': '2px solid black'}) \\\n",
        "    .set_table_styles([dict(selector='th', props=[('font-weight', 'bold'), ('border', '1px solid black')])])\n",
        "\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "_eZiZOZcLABh",
        "outputId": "0fa0834a-8d9e-4f03-b950-5284851651f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fe943cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e1eea th {\n",
              "  font-weight: bold;\n",
              "  border: 1px solid black;\n",
              "}\n",
              "#T_e1eea_row0_col0, #T_e1eea_row0_col1, #T_e1eea_row0_col2, #T_e1eea_row0_col3, #T_e1eea_row0_col4, #T_e1eea_row0_col5, #T_e1eea_row1_col0, #T_e1eea_row1_col1, #T_e1eea_row1_col2, #T_e1eea_row1_col3, #T_e1eea_row1_col4, #T_e1eea_row1_col5, #T_e1eea_row2_col0, #T_e1eea_row2_col1, #T_e1eea_row2_col2, #T_e1eea_row2_col3, #T_e1eea_row2_col4, #T_e1eea_row2_col5 {\n",
              "  font-weight: bold;\n",
              "  border: 2px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e1eea\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e1eea_level0_col0\" class=\"col_heading level0 col0\" >1RNN</th>\n",
              "      <th id=\"T_e1eea_level0_col1\" class=\"col_heading level0 col1\" >1-Bi-RNN</th>\n",
              "      <th id=\"T_e1eea_level0_col2\" class=\"col_heading level0 col2\" >2-Bi-RNN</th>\n",
              "      <th id=\"T_e1eea_level0_col3\" class=\"col_heading level0 col3\" >1LSTM</th>\n",
              "      <th id=\"T_e1eea_level0_col4\" class=\"col_heading level0 col4\" >1Bi-LSTM</th>\n",
              "      <th id=\"T_e1eea_level0_col5\" class=\"col_heading level0 col5\" >2Bi-LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e1eea_level0_row0\" class=\"row_heading level0 row0\" >Accuracy (%)</th>\n",
              "      <td id=\"T_e1eea_row0_col0\" class=\"data row0 col0\" >0.868</td>\n",
              "      <td id=\"T_e1eea_row0_col1\" class=\"data row0 col1\" >0.879</td>\n",
              "      <td id=\"T_e1eea_row0_col2\" class=\"data row0 col2\" >0.862</td>\n",
              "      <td id=\"T_e1eea_row0_col3\" class=\"data row0 col3\" >0.894</td>\n",
              "      <td id=\"T_e1eea_row0_col4\" class=\"data row0 col4\" >0.902</td>\n",
              "      <td id=\"T_e1eea_row0_col5\" class=\"data row0 col5\" >0.899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e1eea_level0_row1\" class=\"row_heading level0 row1\" >Parameters</th>\n",
              "      <td id=\"T_e1eea_row1_col0\" class=\"data row1 col0\" >10,884</td>\n",
              "      <td id=\"T_e1eea_row1_col1\" class=\"data row1 col1\" >21,764</td>\n",
              "      <td id=\"T_e1eea_row1_col2\" class=\"data row1 col2\" >46,596</td>\n",
              "      <td id=\"T_e1eea_row1_col3\" class=\"data row1 col3\" >42,756</td>\n",
              "      <td id=\"T_e1eea_row1_col4\" class=\"data row1 col4\" >85,508</td>\n",
              "      <td id=\"T_e1eea_row1_col5\" class=\"data row1 col5\" >184,836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e1eea_level0_row2\" class=\"row_heading level0 row2\" >Time cost per epoch (s)</th>\n",
              "      <td id=\"T_e1eea_row2_col0\" class=\"data row2 col0\" >4.365</td>\n",
              "      <td id=\"T_e1eea_row2_col1\" class=\"data row2 col1\" >4.490</td>\n",
              "      <td id=\"T_e1eea_row2_col2\" class=\"data row2 col2\" >4.760</td>\n",
              "      <td id=\"T_e1eea_row2_col3\" class=\"data row2 col3\" >4.586</td>\n",
              "      <td id=\"T_e1eea_row2_col4\" class=\"data row2 col4\" >5.081</td>\n",
              "      <td id=\"T_e1eea_row2_col5\" class=\"data row2 col5\" >5.431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}