{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuPII49Y5pCM",
        "outputId": "355a39d6-af55-474a-b8b1-943038ac9a1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "zxdLGlySOxZ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is the same as in the first file and we change the data loading to fit the new dataset:\n",
        "\n",
        "```\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UprmvnFvbFPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1qRxgRtEZ9B",
        "outputId": "3c155ae5-ff3b-427b-9f67-473b897f477a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(29065, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "Total parameters:  2917254\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.695\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.687\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:05<00:00,  6.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.657\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:12<00:00,  3.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.628\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.600\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:05<00:00,  6.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.575\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.560\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:05<00:00,  6.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.548\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.529\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.518\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:11<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.504\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.494\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.486\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.481\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.470\n",
            "\n",
            "Test Accuracy : 0.708\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.69      0.75      0.72      4961\n",
            "    Positive       0.73      0.66      0.70      5039\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.71      0.71      0.71     10000\n",
            "weighted avg       0.71      0.71      0.71     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[3733 1228]\n",
            " [1691 3348]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1-direction RNN classifier applied to IMDB dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) \n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds1, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds1.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds1 = torch.cat(Y_preds1)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds1.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds1 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds1, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds1 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds1)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds1, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds1))\n",
        "\n",
        "parameters1 = count_parameters(classifier)\n",
        "average_time_per_epoch1 = (end_time - start_time)/EPOCHS\n",
        "accuracy1 = accuracy_score(Y_actual, Y_preds1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11ee109-4de5-4117-f538-a9a7e9d64e04",
        "id": "eXjKHZ-4OyDQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(29065, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "Total parameters:  2928006\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:10<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.689\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.664\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.634\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.607\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.583\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.563\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.542\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.523\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.511\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.494\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.482\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.470\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.459\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.449\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.443\n",
            "\n",
            "Test Accuracy : 0.703\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.69      0.74      0.71      4961\n",
            "    Positive       0.72      0.67      0.69      5039\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.70      0.70      0.70     10000\n",
            "weighted avg       0.70      0.70      0.70     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[3667 1294]\n",
            " [1680 3359]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier applied to IMDB dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) \n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds2, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds2.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds2 = torch.cat(Y_preds2)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds2.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds2 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds2, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds2 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds2)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds2, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds2))\n",
        "\n",
        "parameters2 = count_parameters(classifier)\n",
        "average_time_per_epoch2 = (end_time - start_time)/EPOCHS\n",
        "accuracy2 = accuracy_score(Y_actual, Y_preds2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e556d5cd-442b-4cd7-c379-e8ff26837d98",
        "id": "VnjXOao7TDPQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(29065, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "Total parameters:  2952838\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.688\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.660\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.628\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.601\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.573\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.553\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.533\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.519\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.505\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.487\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.477\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.463\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.451\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.447\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.435\n",
            "\n",
            "Test Accuracy : 0.707\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.71      0.70      0.70      4961\n",
            "    Positive       0.71      0.72      0.71      5039\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.71      0.71      0.71     10000\n",
            "weighted avg       0.71      0.71      0.71     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[3449 1512]\n",
            " [1415 3624]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction RNN classifier with 2 layers applied to IMDB dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) \n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.rnn(embeddings)\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward RNN\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward RNN (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds3, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds3.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds3 = torch.cat(Y_preds3)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds3.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds3 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds3, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds3 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds3)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds3, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds3))\n",
        "\n",
        "parameters3 = count_parameters(classifier)\n",
        "average_time_per_epoch3 = (end_time - start_time)/EPOCHS\n",
        "accuracy3 = accuracy_score(Y_actual, Y_preds3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bc2dfd-6054-4148-8f17-9b28d98477ab",
        "id": "KFMUmOY6WjcP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(29065, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "Total parameters:  2949126\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.692\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.676\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.627\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.584\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.556\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.537\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:09<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.522\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:10<00:00,  3.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.502\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:11<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.489\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.475\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:09<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.465\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.457\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.442\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.439\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.430\n",
            "\n",
            "Test Accuracy : 0.717\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.70      0.74      0.72      4961\n",
            "    Positive       0.73      0.69      0.71      5039\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.72     10000\n",
            "weighted avg       0.72      0.72      0.72     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[3675 1286]\n",
            " [1547 3492]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A 1 direction LSTM classifier applied to IMDB dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import time \n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) \n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True) # Change in code - RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.lstm(embeddings)\n",
        "        logits = self.linear(output[:,-1])  # The last output of LSTM is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds4, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds4.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds4 = torch.cat(Y_preds4)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds4.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds4 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds4, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds4 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds4)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds4, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds4))\n",
        "\n",
        "parameters4 = count_parameters(classifier)\n",
        "average_time_per_epoch4 = (end_time - start_time)/EPOCHS\n",
        "accuracy4 = accuracy_score(Y_actual, Y_preds4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier applied to IMDB dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) \n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True) # Change in code bidirectional = True, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds5, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds5.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds5 = torch.cat(Y_preds5)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds5.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds5 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds5, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds5 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds5)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds5, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds5))\n",
        "\n",
        "\n",
        "parameters5 = count_parameters(classifier)\n",
        "average_time_per_epoch5 = (end_time - start_time)/EPOCHS\n",
        "accuracy5 = accuracy_score(Y_actual, Y_preds5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOEk-PtaUS8",
        "outputId": "279f714b-f154-40d4-b0e8-ca1e3231ea3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(29065, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "Total parameters:  2991750\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.686\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:09<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.647\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.606\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.574\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.549\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.527\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.510\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.491\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.475\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.458\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.444\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.436\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:11<00:00,  3.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.422\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.413\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.407\n",
            "\n",
            "Test Accuracy : 0.718\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.71      0.73      0.72      4961\n",
            "    Positive       0.73      0.71      0.72      5039\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.72     10000\n",
            "weighted avg       0.72      0.72      0.72     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[3621 1340]\n",
            " [1477 3562]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "A bi-direction LSTM classifier with 2 layers applied to IMDB dataset\n",
        "\n",
        "Download dataset:\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "MAX_WORDS = 25\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "######################################################################\n",
        "# Read dataset files \n",
        "# ------------------\n",
        "\n",
        "\n",
        "imdb_data = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "train_data, test_data = train_test_split(imdb_data, test_size=0.2, random_state=42)\n",
        "\n",
        "target_classes = [\"Negative\", \"Positive\"]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "######################################################################\n",
        "# Data processing \n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# All texts are truncated and padded to MAX_WORDS tokens\n",
        "def collate_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    Y = torch.tensor(Y) \n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
        "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
        "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
        "\n",
        "train_dataset = [(label, text) for text, label in zip(train_data['review'], train_data['sentiment'])]\n",
        "test_dataset = [(label, text) for text, label in zip(test_data['review'], test_data['sentiment'])]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "# Vocabulary includes all tokens with at least 10 occurrences in the texts\n",
        "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "\n",
        "######################################################################\n",
        "# Define the model\n",
        "# ----------------\n",
        "\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(model, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)  # Change in code bidirectional = True, num_layers = 2, RNN to LSTM\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)  # Change in code - multiply by 2 since it's bidirectional\n",
        "        self.hidden_dim = hidden_dim  # Change in code - set the hidden_dim attribute\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, _ = self.lstm(embeddings) # Change in code - rnn to lstm\n",
        "        forward_output = output[:, -1, :self.hidden_dim]  # Change in code - last output of the forward LSTM\n",
        "        backward_output = output[:, 0, self.hidden_dim:]  # Change in code - first output of the backward LSTM (since it starts from the end)\n",
        "        concatenated_output = torch.cat((forward_output, backward_output), dim=1) # Change in code - concatenate the output\n",
        "        logits = self.linear(concatenated_output) # Change in code - The last output of the forward and the first output of the backward is used for sequence classification\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs\n",
        "    \n",
        "######################################################################\n",
        "# Initiate an instance of the model\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
        "# Define loss function and opimization algorithm\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
        "\n",
        "# Count model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('\\nModel:')\n",
        "print(classifier)\n",
        "print('Total parameters: ',count_parameters(classifier))\n",
        "print('\\n\\n')\n",
        "\n",
        "######################################################################\n",
        "# Define functions to train and evaluate the model\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "def EvaluateModel(model, loss_fn, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_actual, Y_preds6, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_actual.append(Y)\n",
        "            Y_preds6.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_actual = torch.cat(Y_actual)\n",
        "        Y_preds6 = torch.cat(Y_preds6)\n",
        "    \n",
        "    # Returns mean loss, actual labels, predicted labels \n",
        "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds6.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs):\n",
        "    for i in range(1, epochs+1):\n",
        "        model.train()\n",
        "        print('Epoch:',i)\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds6 = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds6, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        \n",
        "start_time = time.time()\n",
        "TrainModel(classifier, loss_fn, optimizer, train_loader, EPOCHS)\n",
        "end_time = time.time()\n",
        "\n",
        "######################################################################\n",
        "# Evaluate the model with test dataset\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "_, Y_actual, Y_preds6 = EvaluateModel(classifier, loss_fn, test_loader)\n",
        "\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds6)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds6, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds6))\n",
        "\n",
        "parameters6 = count_parameters(classifier)\n",
        "average_time_per_epoch6 = (end_time - start_time)/EPOCHS\n",
        "accuracy6 = accuracy_score(Y_actual, Y_preds6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e44b217-656c-4b50-e352-16c6069a1085",
        "id": "E3it53CzfhbY"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(29065, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n",
            "Total parameters:  3091078\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.679\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.628\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.591\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:06<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.561\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.538\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.521\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.497\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.479\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.467\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.455\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:10<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.444\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.434\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.429\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:07<00:00,  5.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.422\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:08<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.425\n",
            "\n",
            "Test Accuracy : 0.722\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.72      0.73      0.72      4961\n",
            "    Positive       0.73      0.72      0.72      5039\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.72     10000\n",
            "weighted avg       0.72      0.72      0.72     10000\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[3601 1360]\n",
            " [1416 3623]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of text that were missclasified by all models and the most frequent pair of correct category and wrong prediction. The matrix of the accuracy, parameters and time cost per epoch of all the above models is below this block."
      ],
      "metadata": {
        "id": "N4gNPZH4bwD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = list(zip(Y_preds1, Y_preds2, Y_preds3, Y_preds4, Y_preds5, Y_preds6))\n",
        "\n",
        "\n",
        "misclassified_indices = []\n",
        "for i, (pred1, pred2, pred3, pred4, pred5, pred6) in enumerate(predictions):\n",
        "    if (pred1 != Y_actual[i]) and (pred2 != Y_actual[i]) and (pred3 != Y_actual[i]) and (pred4 != Y_actual[i]) and (pred5 != Y_actual[i]) and (pred6 != Y_actual[i]):\n",
        "        misclassified_indices.append(i)\n",
        "\n",
        "\n",
        "misclassified_counts = {category: 0 for category in np.unique(Y_actual)}\n",
        "\n",
        "\n",
        "pair_counts = {}\n",
        "\n",
        "for idx in misclassified_indices:\n",
        "    correct_category = Y_actual[idx]\n",
        "    misclassified_counts[correct_category] += 1\n",
        "\n",
        "    pair = (correct_category, Y_preds1[idx])  \n",
        "\n",
        "    if pair in pair_counts:\n",
        "        pair_counts[pair] += 1\n",
        "    else:\n",
        "        pair_counts[pair] = 1\n",
        "\n",
        "label_to_category = {\n",
        "    \"0\": \"0:Positive\",\n",
        "    \"1\": \"1:Sports\",\n",
        "}\n",
        "\n",
        "\n",
        "misclassified_index = misclassified_indices[0]\n",
        "\n",
        "\n",
        "misclassified_text = test_dataset[misclassified_index][1]\n",
        "\n",
        "\n",
        "correct_category = Y_actual[misclassified_index]\n",
        "predicted_category = Y_preds1[misclassified_index]\n",
        "\n",
        "\n",
        "print(f\"Misclassified Text (Index: {misclassified_index}):\")\n",
        "print(misclassified_text)\n",
        "print(f\"\\nShould have been classified as: {label_to_category[str(correct_category)]}\")\n",
        "print(f\"\\nWas classified as: {label_to_category[str(predicted_category)]}\\n\")\n",
        "\n",
        "for category in sorted(misclassified_counts.keys()):\n",
        "    category_name = label_to_category[str(category)]\n",
        "    count = misclassified_counts[category]\n",
        "    print(f\"{category_name}: {count} samples\")\n",
        "\n",
        "\n",
        "most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "print(\"\\nThe most frequent pair of correct category and wrong prediction:\")\n",
        "print(f\"Correct category: {label_to_category[str(most_frequent_pair[0])]}, Wrong prediction: {label_to_category[str(most_frequent_pair[1])]}, Occurrences: {pair_counts[most_frequent_pair]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pppj9_OcvpxL",
        "outputId": "d41fbaaa-c81b-4c1e-9baa-457926703091"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Text (Index: 7):\n",
            "Okay, I didn't get the Purgatory thing the first time I watched this episode. It seemed like something significant was going on that I couldn't put my finger on. This time those Costa Mesa fires on TV really caught my attention- and it helped that I was just writing an essay on Inferno! But let me see what HASN'T been discussed yet...<br /><br />A TWOP review mentioned that Tony had 7 flights of stairs to go down because of the broken elevator. Yeah, 7 is a significant number for lots of reasons, especially religious, but here's one more for ya. On a hunch I consulted wikipedia, and guess what Dante divided into 7 levels? Purgatorio. Excluding ante-Purgatory and Paradise. (The stuff at the bottom of the stairs and... what Tony can't get to.) <br /><br />On to the allegedly \"random\" monk-slap scene. As soon as the monks appeared, it fit perfectly in place with Tony trying to get out of Purgatory. You can tell he got worried when that Christian commercial (death, disease, and sin) came on, and he's getting more and more desperate because Christian heaven is looking kinda iffy for him. By the time he meets the monks he's thinking \"hey maybe these guys can help me?\" which sounds like contemplating other religions (e.g. Buddhism) and wondering if some other path could take him to \"salvation\". Not that Tony is necessarily literally thinking about becoming a Buddhist, but it appears Finnerty tried that (and messed up). That slap in the face basically tells Tony there's no quick fix- as in, no, you can't suddenly embrace Buddhism and get out of here. <br /><br />Tony was initially not too concerned about getting to heaven. But at the \"conference entrance\", he realizes that's not going to be so easy for him. At first I saw the name vs. driver's license problem as Tony having led sort of a double life, what with the killing people and sleeping around that he kept secret from most people. He feels free to have an affair with quasi-Melfi because \"he's Kevin Finnerty\". He figures out that he CAN fool some people with KF's cards, like hotel receptionists, but it won't get him out of Purgatory. Those helicopters- the helicopters of Heaven?- are keeping track of him and everything he does.<br /><br />After reading all the theories on \"inFinnerty\", though, it seems like KF's identity is a reminder of the infinite different paths Tony could've taken in his life. Possibly along with the car joke involving Infiniti's that made no sense to me otherwise. Aaaand at that point my brain fizzles out.\n",
            "\n",
            "Should have been classified as: 1:Sports\n",
            "\n",
            "Was classified as: 0:Positive\n",
            "\n",
            "0:Positive: 287 samples\n",
            "1:Sports: 357 samples\n",
            "\n",
            "The most frequent pair of correct category and wrong prediction:\n",
            "Correct category: 1:Sports, Wrong prediction: 0:Positive, Occurrences: 357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results on the new IMDB dataset, we can observe that the 1-layer RNN has an accuracy of 0.708, while the 1-layer bidirectional RNN has a slightly lower accuracy of 0.703. The highest accuracy is achieved by the 2-layer bidirectional LSTM, with an accuracy of 0.722. Similar to the previous dataset, bidirectional models generally outperform their unidirectional counterparts, indicating the advantage of leveraging bidirectional architectures to capture information from both past and future contexts.\n",
        "\n",
        "In terms of model complexity, as we transition from simpler architectures like RNNs to more advanced ones like bidirectional LSTMs, the number of parameters increases. The 1-layer RNN model has the lowest time cost per epoch at 7.625 seconds, while the 2-layer bidirectional LSTM model has the highest time cost per epoch of 8.014 seconds."
      ],
      "metadata": {
        "id": "uUIq1i4nc3gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def format_values(value):\n",
        "    if isinstance(value, float):\n",
        "        if value.is_integer():\n",
        "            return f\"{int(value):,}\"\n",
        "        else:\n",
        "            return f\"{value:.3f}\"\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"1RNN\": [accuracy1, parameters1, average_time_per_epoch1],\n",
        "    \"1-Bi-RNN\": [accuracy2, parameters2, average_time_per_epoch2],\n",
        "    \"2-Bi-RNN\": [accuracy3, parameters3, average_time_per_epoch3],\n",
        "    \"1LSTM\": [accuracy4, parameters4, average_time_per_epoch4],\n",
        "    \"1Bi-LSTM\": [accuracy5, parameters5, average_time_per_epoch5],\n",
        "    \"2Bi-LSTM\": [accuracy6, parameters6, average_time_per_epoch6],\n",
        "}\n",
        "\n",
        "index = [\"Accuracy (%)\", \"Parameters\", \"Time cost per epoch (s)\"]\n",
        "\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "df = df.applymap(format_values)\n",
        "\n",
        "df = df.style \\\n",
        "    .set_properties(**{'font-weight': 'bold', 'border': '2px solid black'}) \\\n",
        "    .set_table_styles([dict(selector='th', props=[('font-weight', 'bold'), ('border', '1px solid black')])])\n",
        "\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "_eZiZOZcLABh",
        "outputId": "54767f14-039d-4c1b-c5c5-82404679d81c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc486ece350>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_5f40d th {\n",
              "  font-weight: bold;\n",
              "  border: 1px solid black;\n",
              "}\n",
              "#T_5f40d_row0_col0, #T_5f40d_row0_col1, #T_5f40d_row0_col2, #T_5f40d_row0_col3, #T_5f40d_row0_col4, #T_5f40d_row0_col5, #T_5f40d_row1_col0, #T_5f40d_row1_col1, #T_5f40d_row1_col2, #T_5f40d_row1_col3, #T_5f40d_row1_col4, #T_5f40d_row1_col5, #T_5f40d_row2_col0, #T_5f40d_row2_col1, #T_5f40d_row2_col2, #T_5f40d_row2_col3, #T_5f40d_row2_col4, #T_5f40d_row2_col5 {\n",
              "  font-weight: bold;\n",
              "  border: 2px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_5f40d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_5f40d_level0_col0\" class=\"col_heading level0 col0\" >1RNN</th>\n",
              "      <th id=\"T_5f40d_level0_col1\" class=\"col_heading level0 col1\" >1-Bi-RNN</th>\n",
              "      <th id=\"T_5f40d_level0_col2\" class=\"col_heading level0 col2\" >2-Bi-RNN</th>\n",
              "      <th id=\"T_5f40d_level0_col3\" class=\"col_heading level0 col3\" >1LSTM</th>\n",
              "      <th id=\"T_5f40d_level0_col4\" class=\"col_heading level0 col4\" >1Bi-LSTM</th>\n",
              "      <th id=\"T_5f40d_level0_col5\" class=\"col_heading level0 col5\" >2Bi-LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_5f40d_level0_row0\" class=\"row_heading level0 row0\" >Accuracy (%)</th>\n",
              "      <td id=\"T_5f40d_row0_col0\" class=\"data row0 col0\" >0.708</td>\n",
              "      <td id=\"T_5f40d_row0_col1\" class=\"data row0 col1\" >0.703</td>\n",
              "      <td id=\"T_5f40d_row0_col2\" class=\"data row0 col2\" >0.707</td>\n",
              "      <td id=\"T_5f40d_row0_col3\" class=\"data row0 col3\" >0.717</td>\n",
              "      <td id=\"T_5f40d_row0_col4\" class=\"data row0 col4\" >0.718</td>\n",
              "      <td id=\"T_5f40d_row0_col5\" class=\"data row0 col5\" >0.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5f40d_level0_row1\" class=\"row_heading level0 row1\" >Parameters</th>\n",
              "      <td id=\"T_5f40d_row1_col0\" class=\"data row1 col0\" >2,917,254</td>\n",
              "      <td id=\"T_5f40d_row1_col1\" class=\"data row1 col1\" >2,928,006</td>\n",
              "      <td id=\"T_5f40d_row1_col2\" class=\"data row1 col2\" >2,952,838</td>\n",
              "      <td id=\"T_5f40d_row1_col3\" class=\"data row1 col3\" >2,949,126</td>\n",
              "      <td id=\"T_5f40d_row1_col4\" class=\"data row1 col4\" >2,991,750</td>\n",
              "      <td id=\"T_5f40d_row1_col5\" class=\"data row1 col5\" >3,091,078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5f40d_level0_row2\" class=\"row_heading level0 row2\" >Time cost per epoch (s)</th>\n",
              "      <td id=\"T_5f40d_row2_col0\" class=\"data row2 col0\" >7.625</td>\n",
              "      <td id=\"T_5f40d_row2_col1\" class=\"data row2 col1\" >7.263</td>\n",
              "      <td id=\"T_5f40d_row2_col2\" class=\"data row2 col2\" >7.148</td>\n",
              "      <td id=\"T_5f40d_row2_col3\" class=\"data row2 col3\" >7.995</td>\n",
              "      <td id=\"T_5f40d_row2_col4\" class=\"data row2 col4\" >7.575</td>\n",
              "      <td id=\"T_5f40d_row2_col5\" class=\"data row2 col5\" >8.014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}